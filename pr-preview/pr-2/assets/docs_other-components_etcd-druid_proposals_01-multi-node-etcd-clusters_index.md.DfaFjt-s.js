import{_ as t,c as a,o,a2 as i}from"./chunks/framework.B8WFj13S.js";const s="/documentation/pr-preview/pr-2/assets/01-multi-node-etcd.D7d1KpmV.png",r="/documentation/pr-preview/pr-2/assets/01-etcd-member-initialization-sequence.D_5qM_yS.png",n="/documentation/pr-preview/pr-2/assets/01-etcd-backup-restore-work-flows-life-cycle.WR880n_B.png",f=JSON.parse('{"title":"01 Multi Node Etcd Clusters","description":"","frontmatter":{"github_repo":"https://github.com/gardener/etcd-druid","github_subdir":"docs/proposals","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters.md","to":"01-multi-node-etcd-clusters.md"},"title":"01 Multi Node Etcd Clusters","prev":false,"next":false},"headers":[],"relativePath":"docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/index.md","filePath":"docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters.md","lastUpdated":null}'),d={name:"docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/index.md"};function l(c,e,h,u,p,m){return o(),a("div",null,e[0]||(e[0]=[i('<h1 id="dep-01-multi-node-etcd-cluster-instances-via-etcd-druid" tabindex="-1">DEP-01: Multi-node etcd cluster instances via etcd-druid <a class="header-anchor" href="#dep-01-multi-node-etcd-cluster-instances-via-etcd-druid" aria-label="Permalink to &quot;DEP-01: Multi-node etcd cluster instances via etcd-druid&quot;">​</a></h1><p>This document proposes an approach (along with some alternatives) to support provisioning and management of multi-node etcd cluster instances via <a href="https://github.com/gardener/etcd-druid" target="_blank" rel="noreferrer">etcd-druid</a> and <a href="https://github.com/gardener/etcd-backup-restore" target="_blank" rel="noreferrer">etcd-backup-restore</a>.</p><h2 id="goal" tabindex="-1">Goal <a class="header-anchor" href="#goal" aria-label="Permalink to &quot;Goal&quot;">​</a></h2><ul><li>Enhance etcd-druid and etcd-backup-restore to support provisioning and management of multi-node etcd cluster instances within a single Kubernetes cluster.</li><li>The etcd CRD interface should be simple to use. It should preferably work with just setting the <code>spec.replicas</code> field to the desired value and should not require any more configuration in the CRD than currently required for the single-node etcd instances. The <code>spec.replicas</code> field is part of the <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource" target="_blank" rel="noreferrer"><code>scale</code> sub-resource</a> <a href="https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/api/v1alpha1/etcd_types.go#L299" target="_blank" rel="noreferrer">implementation</a> in <code>Etcd</code> CRD.</li><li>The single-node and multi-node scenarios must be automatically identified and managed by <code>etcd-druid</code> and <code>etcd-backup-restore</code>.</li><li>The etcd clusters (single-node or multi-node) managed by <code>etcd-druid</code> and <code>etcd-backup-restore</code> must automatically recover from failures (even quorum loss) and disaster (e.g. etcd member persistence/data loss) as much as possible.</li><li>It must be possible to dynamically scale an etcd cluster horizontally (even between single-node and multi-node scenarios) by simply scaling the <code>Etcd</code> scale sub-resource.</li><li>It must be possible to (optionally) schedule the individual members of an etcd clusters on different nodes or even infrastructure availability zones (within the hosting Kubernetes cluster).</li></ul><p>Though this proposal tries to cover most aspects related to single-node and multi-node etcd clusters, there are some more points that are not goals for this document but are still in the scope of either etcd-druid/etcd-backup-restore and/or gardener. In such cases, a high-level description of how they can be <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work">addressed in the future</a> are mentioned at the end of the document.</p><h2 id="background-and-motivation" tabindex="-1">Background and Motivation <a class="header-anchor" href="#background-and-motivation" aria-label="Permalink to &quot;Background and Motivation&quot;">​</a></h2><h3 id="single-node-etcd-cluster" tabindex="-1">Single-node etcd cluster <a class="header-anchor" href="#single-node-etcd-cluster" aria-label="Permalink to &quot;Single-node etcd cluster&quot;">​</a></h3><p>At present, <code>etcd-druid</code> supports only single-node etcd cluster instances. The advantages of this approach are given below.</p><ul><li>The problem domain is smaller. There are no leader election and quorum related issues to be handled. It is simpler to setup and manage a single-node etcd cluster.</li><li>Single-node etcd clusters instances have <a href="https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size" target="_blank" rel="noreferrer">less request latency</a> than multi-node etcd clusters because there is no requirement to replicate the changes to the other members before committing the changes.</li><li><code>etcd-druid</code> provisions etcd cluster instances as pods (actually as <code>statefulsets</code>) in a Kubernetes cluster and Kubernetes is quick (&lt;<code>20s</code>) to restart container/pods if they go down.</li><li>Also, <code>etcd-druid</code> is currently only used by gardener to provision etcd clusters to act as back-ends for Kubernetes control-planes and Kubernetes control-plane components (<code>kube-apiserver</code>, <code>kubelet</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code> etc.) can tolerate etcd going down and recover when it comes back up.</li><li>Single-node etcd clusters incur less cost (CPU, memory and storage)</li><li>It is easy to cut-off client requests if backups fail by using <a href="https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/charts/etcd/templates/etcd-statefulset.yaml#L54-L62" target="_blank" rel="noreferrer"><code>readinessProbe</code> on the <code>etcd-backup-restore</code> healthz endpoint</a> to minimize the gap between the latest revision and the backup revision.</li></ul><p>The disadvantages of using single-node etcd clusters are given below.</p><ul><li>The <a href="https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/design.md#workflow" target="_blank" rel="noreferrer">database verification</a> step by <code>etcd-backup-restore</code> can introduce additional delays whenever etcd container/pod restarts (in total ~<code>20-25s</code>). This can be much longer if a database restoration is required. Especially, if there are incremental snapshots that need to be replayed (this can be mitigated by <a href="https://github.com/gardener/etcd-druid/issues/88" target="_blank" rel="noreferrer">compacting the incremental snapshots in the background</a>).</li><li>Kubernetes control-plane components can go into <code>CrashloopBackoff</code> if etcd is down for some time. This is mitigated by the <a href="https://github.com/gardener/gardener/blob/9e4a809008fb122a6d02045adc08b9c98b5cd564/charts/seed-bootstrap/charts/dependency-watchdog/templates/endpoint-configmap.yaml#L29-L41" target="_blank" rel="noreferrer">dependency-watchdog</a>. But Kubernetes control-plane components require a lot of resources and create a lot of load on the etcd cluster and the apiserver when they come out of <code>CrashloopBackoff</code>. Especially, in medium or large sized clusters (&gt; <code>20</code> nodes).</li><li>Maintenance operations such as updates to etcd (and updates to <code>etcd-druid</code> of <code>etcd-backup-restore</code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods are disruptive because they cause etcd pods to be restarted. The vertical scaling of etcd pods is somewhat mitigated during scale down by doing it only during the target clusters&#39; <a href="https://github.com/gardener/gardener/blob/86aa30dfd095f7960ae50a81d2cee27c0d18408b/charts/seed-controlplane/charts/etcd/templates/etcd-hvpa.yaml#L53" target="_blank" rel="noreferrer">maintenance window</a>. But scale up is still disruptive.</li><li>We currently use some form of elastic storage (via <code>persistentvolumeclaims</code>) for storing which have some upper-bounds on the I/O latency and throughput. This can be potentially be a problem for large clusters (&gt; <code>220</code> nodes). Also, some cloud providers (e.g. Azure) take a long time to attach/detach volumes to and from machines which increases the down time to the Kubernetes components that depend on etcd. It is difficult to use ephemeral/local storage (to achieve better latency/throughput as well as to circumvent volume attachment/detachment) for single-node etcd cluster instances.</li></ul><h3 id="multi-node-etcd-cluster" tabindex="-1">Multi-node etcd-cluster <a class="header-anchor" href="#multi-node-etcd-cluster" aria-label="Permalink to &quot;Multi-node etcd-cluster&quot;">​</a></h3><p>The advantages of introducing support for multi-node etcd clusters via <code>etcd-druid</code> are below.</p><ul><li>Multi-node etcd cluster is highly-available. It can tolerate disruption to individual etcd pods as long as the quorum is not lost (i.e. more than half the etcd member pods are healthy and ready).</li><li>Maintenance operations such as updates to etcd (and updates to <code>etcd-druid</code> of <code>etcd-backup-restore</code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods can be done non-disruptively by <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/" target="_blank" rel="noreferrer">respecting <code>poddisruptionbudgets</code></a> for the various multi-node etcd cluster instances hosted on that cluster.</li><li>Kubernetes control-plane components do not see any etcd cluster downtime unless quorum is lost (which is expected to be lot less frequent than current frequency of etcd container/pod restarts).</li><li>We can consider using ephemeral/local storage for multi-node etcd cluster instances because individual member restarts can afford to take time to restore from backup before (re)joining the etcd cluster because the remaining members serve the requests in the meantime.</li><li>High-availability across availability zones is also possible by specifying (anti)affinity for the etcd pods (possibly via <a href="https://github.com/gardener/kupid" target="_blank" rel="noreferrer"><code>kupid</code></a>).</li></ul><p>Some disadvantages of using multi-node etcd clusters due to which it might still be desirable, in some cases, to continue to use single-node etcd cluster instances in the gardener context are given below.</p><ul><li>Multi-node etcd cluster instances are more complex to manage. The problem domain is larger including the following. <ul><li>Leader election</li><li>Quorum loss</li><li>Managing rolling changes</li><li>Backups to be taken from only the leading member.</li><li>More complex to cut-off client requests if backups fail to minimize the gap between the latest revision and the backup revision is under control.</li></ul></li><li>Multi-node etcd cluster instances incur more cost (CPU, memory and storage).</li></ul><h3 id="dynamic-multi-node-etcd-cluster" tabindex="-1">Dynamic multi-node etcd cluster <a class="header-anchor" href="#dynamic-multi-node-etcd-cluster" aria-label="Permalink to &quot;Dynamic multi-node etcd cluster&quot;">​</a></h3><p>Though it is <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#non-goal">not part of this proposal</a>, it is conceivable to convert a single-node etcd cluster into a multi-node etcd cluster temporarily to perform some disruptive operation (etcd, <code>etcd-backup-restore</code> or <code>etcd-druid</code> updates, etcd cluster vertical scaling and perhaps even node rollout) and convert it back to a single-node etcd cluster once the disruptive operation has been completed. This will necessarily still involve a down-time because scaling from a single-node etcd cluster to a three-node etcd cluster will involve etcd pod restarts, it is still probable that it can be managed with a shorter down time than we see at present for single-node etcd clusters (on the other hand, converting a three-node etcd cluster to five node etcd cluster can be non-disruptive).</p><p>This is <em>definitely not</em> to argue in favour of such a dynamic approach in all cases (eventually, if/when dynamic multi-node etcd clusters are supported). On the contrary, it makes sense to make use of <em>static</em> (fixed in size) multi-node etcd clusters for production scenarios because of the high-availability.</p><h2 id="prior-art" tabindex="-1">Prior Art <a class="header-anchor" href="#prior-art" aria-label="Permalink to &quot;Prior Art&quot;">​</a></h2><h3 id="etcd-operator-from-coreos" tabindex="-1">ETCD Operator from CoreOS <a class="header-anchor" href="#etcd-operator-from-coreos" aria-label="Permalink to &quot;ETCD Operator from CoreOS&quot;">​</a></h3><blockquote><p><a href="https://github.com/coreos/etcd-operator#etcd-operator" target="_blank" rel="noreferrer">etcd operator</a></p><p><a href="https://github.com/coreos/etcd-operator#project-status-archived" target="_blank" rel="noreferrer">Project status: archived</a></p><p>This project is no longer actively developed or maintained. The project exists here for historical reference. If you are interested in the future of the project and taking over stewardship, please contact <a href="mailto:etcd-dev@googlegroups.com" target="_blank" rel="noreferrer">etcd-dev@googlegroups.com</a>.</p></blockquote><h3 id="etcdadm-from-kubernetes-sigs" tabindex="-1">etcdadm from kubernetes-sigs <a class="header-anchor" href="#etcdadm-from-kubernetes-sigs" aria-label="Permalink to &quot;etcdadm from kubernetes-sigs&quot;">​</a></h3><blockquote><p><a href="https://github.com/kubernetes-sigs/etcdadm#etcdadm" target="_blank" rel="noreferrer">etcdadm</a> is a command-line tool for operating an etcd cluster. It makes it easy to create a new cluster, add a member to, or remove a member from an existing cluster. Its user experience is inspired by kubeadm.</p></blockquote><p>It is a tool more tailored for manual command-line based management of etcd clusters with no API&#39;s. It also makes no assumptions about the underlying platform on which the etcd clusters are provisioned and hence, doesn&#39;t leverage any capabilities of Kubernetes.</p><h3 id="etcd-cluster-operator-from-improbable-engineering" tabindex="-1">Etcd Cluster Operator from Improbable-Engineering <a class="header-anchor" href="#etcd-cluster-operator-from-improbable-engineering" aria-label="Permalink to &quot;Etcd Cluster Operator from Improbable-Engineering&quot;">​</a></h3><blockquote><p><a href="https://github.com/improbable-eng/etcd-cluster-operator" target="_blank" rel="noreferrer">Etcd Cluster Operator</a></p><p>Etcd Cluster Operator is an Operator for automating the creation and management of etcd inside of Kubernetes. It provides a custom resource definition (CRD) based API to define etcd clusters with Kubernetes resources, and enable management with native Kubernetes tooling._</p></blockquote><p>Out of all the alternatives listed here, this one seems to be the only possible viable alternative. Parts of its design/implementations are similar to some of the approaches mentioned in this proposal. However, we still don&#39;t propose to use it as -</p><ol><li>The project is still in early phase and is not mature enough to be consumed as is in productive scenarios of ours.</li><li>The resotration part is completely different which makes it difficult to adopt as-is and requries lot of re-work with the current restoration semantics with etcd-backup-restore making the usage counter-productive.</li></ol><h2 id="general-approach-to-etcd-cluster-management" tabindex="-1">General Approach to ETCD Cluster Management <a class="header-anchor" href="#general-approach-to-etcd-cluster-management" aria-label="Permalink to &quot;General Approach to ETCD Cluster Management&quot;">​</a></h2><h3 id="bootstrapping" tabindex="-1">Bootstrapping <a class="header-anchor" href="#bootstrapping" aria-label="Permalink to &quot;Bootstrapping&quot;">​</a></h3><p>There are three ways to bootstrap an etcd cluster which are <a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#static" target="_blank" rel="noreferrer">static</a>, <a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#etcd-discovery" target="_blank" rel="noreferrer">etcd discovery</a> and <a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#dns-discovery" target="_blank" rel="noreferrer">DNS discovery</a>. Out of these, the static way is the simplest (and probably faster to bootstrap the cluster) and has the least external dependencies. Hence, it is preferred in this proposal. But it requires that the initial (during bootstrapping) etcd cluster size (number of members) is already known before bootstrapping and that all of the members are already addressable (DNS,IP,TLS etc.). Such information needs to be passed to the individual members during startup using the following static configuration.</p><ul><li>ETCD_INITIAL_CLUSTER <ul><li>The list of peer URLs including all the members. This must be the same as the advertised peer URLs configuration. This can also be passed as <code>initial-cluster</code> flag to etcd.</li></ul></li><li>ETCD_INITIAL_CLUSTER_STATE <ul><li>This should be set to <code>new</code> while bootstrapping an etcd cluster.</li></ul></li><li>ETCD_INITIAL_CLUSTER_TOKEN <ul><li>This is a token to distinguish the etcd cluster from any other etcd cluster in the same network.</li></ul></li></ul><h4 id="assumptions" tabindex="-1">Assumptions <a class="header-anchor" href="#assumptions" aria-label="Permalink to &quot;Assumptions&quot;">​</a></h4><ul><li>ETCD_INITIAL_CLUSTER can use DNS instead of IP addresses. We need to verify this by deleting a pod (as against scaling down the statefulset) to ensure that the pod IP changes and see if the recreated pod (by the statefulset controller) re-joins the cluster automatically.</li><li>DNS for the individual members is known or computable. This is true in the case of etcd-druid setting up an etcd cluster using a single statefulset. But it may not necessarily be true in other cases (multiple statefulset per etcd cluster or deployments instead of statefulsets or in the case of etcd cluster with members distributed across more than one Kubernetes cluster.</li></ul><h3 id="adding-a-new-member-to-an-etcd-cluster" tabindex="-1">Adding a new member to an etcd cluster <a class="header-anchor" href="#adding-a-new-member-to-an-etcd-cluster" aria-label="Permalink to &quot;Adding a new member to an etcd cluster&quot;">​</a></h3><p>A <a href="https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#add-a-new-member" target="_blank" rel="noreferrer">new member can be added</a> to an existing etcd cluster instance using the following steps.</p><ol><li>If the latest backup snapshot exists, restore the member&#39;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster. <ol><li>If the latest backup snapshot doesn&#39;t exist or if the latest backup snapshot is not accessible (please see <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">backup failure</a>) and if the cluster itself is quorate, then the new member can be started with an empty data. But this will will be suboptimal because the new member will fetch all the data from the leading member to get up-to-date.</li></ol></li><li>The cluster is informed that a new member is being added using the <a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L40" target="_blank" rel="noreferrer"><code>MemberAdd</code> API</a> including information like the member name and its advertised peer URLs.</li><li>The new etcd member is then started with <code>ETCD_INITIAL_CLUSTER_STATE=existing</code> apart from other required configuration.</li></ol><p>This proposal recommends this approach.</p><h4 id="note" tabindex="-1">Note <a class="header-anchor" href="#note" aria-label="Permalink to &quot;Note&quot;">​</a></h4><ul><li>If there are incremental snapshots (taken by <code>etcd-backup-restore</code>), they cannot be applied because that requires the member to be started in isolation without joining the cluster which is not possible. This is acceptable if the amount of incremental snapshots are managed to be relatively small. This adds one more reason to increase the priority of the issue of <a href="https://github.com/gardener/etcd-druid/issues/88" target="_blank" rel="noreferrer">incremental snapshot compaction</a>.</li><li>There is a time window, between the <code>MemberAdd</code> call and the new member joining the cluster and getting up to date, where the cluster is <a href="https://etcd.io/docs/v3.3.12/learning/learner/#background" target="_blank" rel="noreferrer">vulnerable to leader elections which could be disruptive</a>.</li></ul><h4 id="alternative" tabindex="-1">Alternative <a class="header-anchor" href="#alternative" aria-label="Permalink to &quot;Alternative&quot;">​</a></h4><p>With <code>v3.4</code>, the new <a href="https://etcd.io/docs/v3.3.12/learning/learner/#raft-learner" target="_blank" rel="noreferrer">raft learner approach</a> can be used to mitigate some of the possible disruptions mentioned <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">above</a>. Then the steps will be as follows.</p><ol><li>If the latest backup snapshot exists, restore the member&#39;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.</li><li>The cluster is informed that a new member is being added using the <a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L43" target="_blank" rel="noreferrer"><code>MemberAddAsLearner</code> API</a> including information like the member name and its advertised peer URLs.</li><li>The new etcd member is then started with <code>ETCD_INITIAL_CLUSTER_STATE=existing</code> apart from other required configuration.</li><li>Once the new member (learner) is up to date, it can be promoted to a full voting member by using the <a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L52" target="_blank" rel="noreferrer"><code>MemberPromote</code> API</a></li></ol><p>This approach is new and involves more steps and is not recommended in this proposal. It can be considered in future enhancements.</p><h3 id="managing-failures" tabindex="-1">Managing Failures <a class="header-anchor" href="#managing-failures" aria-label="Permalink to &quot;Managing Failures&quot;">​</a></h3><p>A multi-node etcd cluster may face failures of <a href="https://etcd.io/docs/v3.1.12/op-guide/failures/" target="_blank" rel="noreferrer">diffent kinds</a> during its life-cycle. The actions that need to be taken to manage these failures depend on the failure mode.</p><h4 id="removing-an-existing-member-from-an-etcd-cluster" tabindex="-1">Removing an existing member from an etcd cluster <a class="header-anchor" href="#removing-an-existing-member-from-an-etcd-cluster" aria-label="Permalink to &quot;Removing an existing member from an etcd cluster&quot;">​</a></h4><p>If a member of an etcd cluster becomes unhealthy, it must be explicitly removed from the etcd cluster, as soon as possible. This can be done by using the <a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L46" target="_blank" rel="noreferrer"><code>MemberRemove</code> API</a>. This ensures that only healthy members participate as voting members.</p><p>A member of an etcd cluster may be removed not just for managing failures but also for other reasons such as -</p><ul><li>The etcd cluster is being scaled down. I.e. the cluster size is being reduced</li><li>An existing member is being replaced by a new one for some reason (e.g. upgrades)</li></ul><p>If the majority of the members of the etcd cluster are healthy and the member that is unhealthy/being removed happens to be the <a href="https://etcd.io/docs/v3.1.12/op-guide/failures/#leader-failure" target="_blank" rel="noreferrer">leader</a> at that moment then the etcd cluster will automatically elect a new leader. But if only a minority of etcd clusters are healthy after removing the member then the the cluster will no longer be <a href="https://etcd.io/docs/v3.1.12/op-guide/failures/#majority-failure" target="_blank" rel="noreferrer">quorate</a> and will stop accepting write requests. Such an etcd cluster needs to be recovered via some kind of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">disaster-recovery</a>.</p><h4 id="restarting-an-existing-member-of-an-etcd-cluster" tabindex="-1">Restarting an existing member of an etcd cluster <a class="header-anchor" href="#restarting-an-existing-member-of-an-etcd-cluster" aria-label="Permalink to &quot;Restarting an existing member of an etcd cluster&quot;">​</a></h4><p>If the existing member of an etcd cluster restarts and retains an uncorrupted data directory after the restart, then it can simply re-join the cluster as an existing member without any API calls or configuration changes. This is because the relevant metadata (including member ID and cluster ID) are <a href="https://etcd.io/docs/v2/admin_guide/#lifecycle" target="_blank" rel="noreferrer">maintained in the write ahead logs</a>. However, if it doesn&#39;t retain an uncorrupted data directory after the restart, then it must first be <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">removed</a> and <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">added</a> as a new member.</p><h4 id="recovering-an-etcd-cluster-from-failure-of-majority-of-members" tabindex="-1">Recovering an etcd cluster from failure of majority of members <a class="header-anchor" href="#recovering-an-etcd-cluster-from-failure-of-majority-of-members" aria-label="Permalink to &quot;Recovering an etcd cluster from failure of majority of members&quot;">​</a></h4><p>If a majority of members of an etcd cluster fail but if they retain their uncorrupted data directory then they can be simply restarted and they will re-form the existing etcd cluster when they come up. However, if they do not retain their uncorrupted data directory, then the etcd cluster must be <a href="https://etcd.io/docs/v3.4.0/op-guide/recovery/#restoring-a-cluster" target="_blank" rel="noreferrer">recovered from latest snapshot in the backup</a>. This is very similar to <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping</a> with the additional initial step of restoring the latest snapshot in each of the members. However, the same <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">limitation</a> about incremental snapshots, as in the case of adding a new member, applies here. But unlike in the case of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">adding a new member</a>, not applying incremental snapshots is not acceptable in the case of etcd cluster recovery. Hence, if incremental snapshots are required to be applied, the etcd cluster must be <a href="https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#restart-cluster-from-majority-failure" target="_blank" rel="noreferrer">recovered</a> in the following steps.</p><ol><li>Restore a new single-member cluster using the latest snapshot.</li><li>Apply incremental snapshots on the single-member cluster.</li><li>Take a full snapshot which can now be used while adding the remaining members.</li><li><a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add</a> new members using the latest snapshot created in the step above.</li></ol><h2 id="kubernetes-context" tabindex="-1">Kubernetes Context <a class="header-anchor" href="#kubernetes-context" aria-label="Permalink to &quot;Kubernetes Context&quot;">​</a></h2><ul><li>Users will provision an etcd cluster in a Kubernetes cluster by creating an etcd CRD resource instance.</li><li>A multi-node etcd cluster is indicated if the <code>spec.replicas</code> field is set to any value greater than 1. The etcd-druid will add validation to ensure that the <code>spec.replicas</code> value is an odd number according to the requirements of etcd.</li><li>The etcd-druid controller will provision a statefulset with the etcd main container and the etcd-backup-restore sidecar container. It will pass on the <code>spec.replicas</code> field from the etcd resource to the statefulset. It will also supply the right pre-computed configuration to both the containers.</li><li>The statefulset controller will create the pods based on the pod template in the statefulset spec and these individual pods will be the members that form the etcd cluster.</li></ul><p><img src="'+s+`" alt="Component diagram"></p><p>This approach makes it possible to satisfy the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumption">assumption</a> that the DNS for the individual members of the etcd cluster must be known/computable. This can be achieved by using a <code>headless</code> service (along with the statefulset) for each etcd cluster instance. Then we can address individual pods/etcd members via the predictable DNS name of <code>&lt;statefulset_name&gt;-{0|1|2|3|…|n}.&lt;headless_service_name&gt;</code> from within the Kubernetes namespace (or from outside the Kubernetes namespace by appending <code>.&lt;namespace&gt;.svc.&lt;cluster_domain&gt; suffix)</code>. The etcd-druid controller can compute the above configurations automatically based on the <code>spec.replicas</code> in the etcd resource.</p><p>This proposal recommends this approach.</p><h4 id="alternative-1" tabindex="-1">Alternative <a class="header-anchor" href="#alternative-1" aria-label="Permalink to &quot;Alternative&quot;">​</a></h4><p>One statefulset is used for each member (instead of one statefulset for all members). While this approach gives a flexibility to have different pod specifications for the individual members, it makes managing the individual members (e.g. rolling updates) more complicated. Hence, this approach is not recommended.</p><h2 id="etcd-configuration" tabindex="-1">ETCD Configuration <a class="header-anchor" href="#etcd-configuration" aria-label="Permalink to &quot;ETCD Configuration&quot;">​</a></h2><p>As mentioned in the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management">general approach section</a>, there are differences in the configuration that needs to be passed to individual members of an etcd cluster in different scenarios such as <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping</a>, <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">adding</a> a new member, <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">removing</a> a member, <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restarting</a> an existing member etc. Managing such differences in configuration for individual pods of a statefulset is tricky in the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">recommended approach</a> of using a single statefulset to manage all the member pods of an etcd cluster. This is because statefulset uses the same pod template for all its pods.</p><p>The recommendation is for <code>etcd-druid</code> to provision the base configuration template in a <code>ConfigMap</code> which is passed to all the pods via the pod template in the <code>StatefulSet</code>. The <code>initialization</code> flow of <code>etcd-backup-restore</code> (which is invoked every time the etcd container is (re)started) is then enhanced to generate the customized etcd configuration for the corresponding member pod (in a shared <em>volume</em> between etcd and the backup-restore containers) based on the supplied template configuration. This will require that <code>etcd-backup-restore</code> will have to have a mechanism to detect which scenario listed <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">above</a> applies during any given member container/pod restart.</p><h3 id="alternative-2" tabindex="-1">Alternative <a class="header-anchor" href="#alternative-2" aria-label="Permalink to &quot;Alternative&quot;">​</a></h3><p>As mentioned <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1">above</a>, one statefulset is used for each member of the etcd cluster. Then different configuration (generated directly by <code>etcd-druid</code>) can be passed in the pod templates of the different statefulsets. Though this approach is advantageous in the context of managing the different configuration, it is not recommended in this proposal because it makes the rest of the management (e.g. rolling updates) more complicated.</p><h2 id="data-persistence" tabindex="-1">Data Persistence <a class="header-anchor" href="#data-persistence" aria-label="Permalink to &quot;Data Persistence&quot;">​</a></h2><p>The type of persistence used to store etcd data (including the member ID and cluster ID) has an impact on the steps that are needed to be taken when the member pods or containers (<a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">minority</a> of them or <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">majority</a>) need to be recovered.</p><h3 id="persistent" tabindex="-1">Persistent <a class="header-anchor" href="#persistent" aria-label="Permalink to &quot;Persistent&quot;">​</a></h3><p>Like the single-node case, <code>persistentvolumes</code> can be used to persist ETCD data for all the member pods. The individual member pods then get their own <code>persistentvolumes</code>. The advantage is that individual members retain their member ID across pod restarts and even pod deletion/recreation across Kubernetes nodes. This means that member pods that crash (or are unhealthy) can be <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restarted</a> automatically (by configuring <code>livenessProbe</code>) and they will re-join the etcd cluster using their existing member ID without any need for explicit etcd cluster management).</p><p>The disadvantages of this approach are as follows.</p><ul><li>The number of persistentvolumes increases linearly with the cluster size which is a cost-related concern.</li><li>Network-mounted persistentvolumes might eventually become a performance bottleneck under heavy load for a latency-sensitive component like ETCD.</li><li><a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster">Volume attach/detach issues</a> when associated with etcd cluster instances cause downtimes to the target shoot clusters that are backed by those etcd cluster instances.</li></ul><h3 id="ephemeral" tabindex="-1">Ephemeral <a class="header-anchor" href="#ephemeral" aria-label="Permalink to &quot;Ephemeral&quot;">​</a></h3><p>The ephemeral volumes use-case is considered as an optimization and may be planned as a follow-up action.</p><h4 id="disk" tabindex="-1">Disk <a class="header-anchor" href="#disk" aria-label="Permalink to &quot;Disk&quot;">​</a></h4><p>Ephemeral persistence can be achieved in Kubernetes by using either <a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir" target="_blank" rel="noreferrer"><code>emptyDir</code></a> volumes or <a href="https://kubernetes.io/docs/concepts/storage/volumes/#local" target="_blank" rel="noreferrer"><code>local</code> persistentvolumes</a> to persist ETCD data. The advantages of this approach are as follows.</p><ul><li>Potentially faster disk I/O.</li><li>The number of persistent volumes does not increase linearly with the cluster size (at least not technically).</li><li>Issues related volume attachment/detachment can be avoided.</li></ul><p>The main disadvantage of using ephemeral persistence is that the individual members may retain their identity and data across container restarts but not across pod deletion/recreation across Kubernetes nodes. If the data is lost then on restart of the member pod, the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">older member (represented by the container) has to be removed and a new member has to be added</a>.</p><p>Using <code>emptyDir</code> ephemeral persistence has the disadvantage that the volume doesn&#39;t have its own identity. So, if the member pod is recreated but scheduled on the same node as before then it will not retain the identity as the persistence is lost. But it has the advantage that scheduling of pods is unencumbered especially during pod recreation as they are free to be scheduled anywhere.</p><p>Using <code>local</code> persistentvolumes has the advantage that the volume has its own indentity and hence, a recreated member pod will retain its identity if scheduled on the same node. But it has the disadvantage of tying down the member pod to a node which is a problem if the node becomes unhealthy requiring etcd druid to take additional actions (such as deleting the local persistent volume).</p><p>Based on these constraints, if ephemeral persistence is opted for, it is recommended to use <code>emptyDir</code> ephemeral persistence.</p><h4 id="in-memory" tabindex="-1">In-memory <a class="header-anchor" href="#in-memory" aria-label="Permalink to &quot;In-memory&quot;">​</a></h4><p>In-memory ephemeral persistence can be achieved in Kubernetes by using <code>emptyDir</code> with <a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir" target="_blank" rel="noreferrer"><code>medium: Memory</code></a>. In this case, a <code>tmpfs</code> (RAM-backed file-system) volume will be used. In addition to the advantages of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral persistence</a>, this approach can achieve the fastest possible <em>disk I/O</em>. Similarly, in addition to the disadvantages of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral persistence</a>, in-memory persistence has the following additional disadvantages.</p><ul><li>More memory required for the individual member pods.</li><li>Individual members may not at all retain their data and identity across container restarts let alone across pod restarts/deletion/recreation across Kubernetes nodes. I.e. every time an etcd container restarts, <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">the old member (represented by the container) will have to be removed and a new member has to be added</a>.</li></ul><h3 id="how-to-detect-if-valid-metadata-exists-in-an-etcd-member" tabindex="-1">How to detect if valid metadata exists in an etcd member <a class="header-anchor" href="#how-to-detect-if-valid-metadata-exists-in-an-etcd-member" aria-label="Permalink to &quot;How to detect if valid metadata exists in an etcd member&quot;">​</a></h3><p>Since the likelyhood of a member not having valid metadata in the WAL files is much more likely in the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral</a> persistence scenario, one option is to pass the information that ephemeral persistence is being used to the <code>etcd-backup-restore</code> sidecar (say, via command-line flags or environment variables).</p><p>But in principle, it might be better to determine this from the WAL files directly so that the possibility of corrupted WAL files also gets handled correctly. To do this, the <a href="https://github.com/etcd-io/etcd/tree/main/server/storage/wal" target="_blank" rel="noreferrer">wal</a> package has <a href="https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L324-L326" target="_blank" rel="noreferrer">some</a> <a href="https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L429-L548" target="_blank" rel="noreferrer">functions</a> that might be useful.</p><h4 id="recommendation" tabindex="-1">Recommendation <a class="header-anchor" href="#recommendation" aria-label="Permalink to &quot;Recommendation&quot;">​</a></h4><p>It might be possible that using the <a href="https://github.com/etcd-io/etcd/tree/main/server/storage/wal" target="_blank" rel="noreferrer">wal</a> package for verifying if valid metadata exists might be performance intensive. So, the performance impact needs to be measured. If the performance impact is acceptable (both in terms of resource usage and time), it is recommended to use this way to verify if the member contains valid metadata. Otherwise, alternatives such as a simple check that WAL folder exists coupled with the static information about use of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent</a> or <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral</a> storage might be considered.</p><h3 id="how-to-detect-if-valid-data-exists-in-an-etcd-member" tabindex="-1">How to detect if valid data exists in an etcd member <a class="header-anchor" href="#how-to-detect-if-valid-data-exists-in-an-etcd-member" aria-label="Permalink to &quot;How to detect if valid data exists in an etcd member&quot;">​</a></h3><p>The <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization">initialization sequence</a> in <code>etcd-backup-restore</code> already includes <a href="https://github.com/gardener/etcd-backup-restore/blob/c98f76c7c55f7d1039687cc293536d7caf893ba5/pkg/initializer/validator/datavalidator.go#L78-L94" target="_blank" rel="noreferrer">database verification</a>. This would suffice to determine if the member has valid data.</p><h3 id="recommendation-1" tabindex="-1">Recommendation <a class="header-anchor" href="#recommendation-1" aria-label="Permalink to &quot;Recommendation&quot;">​</a></h3><p>Though <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral</a> persistence has performance and logistics advantages, it is recommended to start with <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent</a> data for the member pods. In addition to the reasons and concerns listed above, there is also the additional concern that in case of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">backup failure</a>, the risk of additional data loss is a bit higher if ephemeral persistence is used (simultaneous quoram loss is sufficient) when compared to persistent storage (simultaenous quorum loss with majority persistence loss is needed). The risk might still be acceptable but the idea is to gain experience about how frequently member containers/pods get restarted/recreated, how frequently leader election happens among members of an etcd cluster and how frequently etcd clusters lose quorum. Based on this experience, we can move towards using <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral</a> (perhaps even <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">in-memory</a>) persistence for the member pods.</p><h2 id="separating-peer-and-client-traffic" tabindex="-1">Separating peer and client traffic <a class="header-anchor" href="#separating-peer-and-client-traffic" aria-label="Permalink to &quot;Separating peer and client traffic&quot;">​</a></h2><p>The current single-node ETCD cluster implementation in <code>etcd-druid</code> and <code>etcd-backup-restore</code> uses a single <code>service</code> object to act as the entry point for the client traffic. There is no separation or distinction between the client and peer traffic because there is not much benefit to be had by making that distinction.</p><p>In the multi-node ETCD cluster scenario, it makes sense to distinguish between and separate the peer and client traffic. This can be done by using two <code>services</code>.</p><ul><li>peer <ul><li>To be used for peer communication. This could be a <code>headless</code> service.</li></ul></li><li>client <ul><li>To be used for client communication. This could be a normal <code>ClusterIP</code> service like it is in the single-node case.</li></ul></li></ul><p>The main advantage of this approach is that it makes it possible (if needed) to allow only peer to peer communication while blocking client communication. Such a thing might be required during some phases of some maintenance tasks (manual or automated).</p><h3 id="cutting-off-client-requests" tabindex="-1">Cutting off client requests <a class="header-anchor" href="#cutting-off-client-requests" aria-label="Permalink to &quot;Cutting off client requests&quot;">​</a></h3><p>At present, in the single-node ETCD instances, etcd-druid configures the readinessProbe of the etcd main container to probe the healthz endpoint of the etcd-backup-restore sidecar which considers the status of the latest backup upload in addition to the regular checks about etcd and the side car being up and healthy. This has the effect of setting the etcd main container (and hence the etcd pod) as not ready if the latest backup upload failed. This results in the endpoints controller removing the pod IP address from the endpoints list for the service which eventually cuts off ingress traffic coming into the etcd pod via the etcd client service. The rationale for this is to fail early when the backup upload fails rather than continuing to serve requests while the gap between the last backup and the current data increases which might lead to unacceptably large amount of data loss if disaster strikes.</p><p>This approach will not work in the multi-node scenario because we need the individual member pods to be able to talk to each other to maintain the cluster quorum when backup upload fails but need to cut off only client ingress traffic.</p><p>It is recommended to separate the backup health condition tracking taking appropriate remedial actions. With that, the backup health condition tracking is now separated to the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions"><code>BackupReady</code> condition</a> in the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status"><code>Etcd</code> resource <code>status</code></a> and the cutting off of client traffic (which could now be done for more reasons than failed backups) can be achieved in a different way described <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">below</a>.</p><h4 id="manipulating-client-service-podselector" tabindex="-1">Manipulating Client Service podSelector <a class="header-anchor" href="#manipulating-client-service-podselector" aria-label="Permalink to &quot;Manipulating Client Service podSelector&quot;">​</a></h4><p>The client traffic can be cut off by updating (manually or automatically by some component) the <code>podSelector</code> of the client service to add an additional label (say, unhealthy or disabled) such that the <code>podSelector</code> no longer matches the member pods created by the statefulset. This will result in the client ingress traffic being cut off. The peer service is left unmodified so that peer communication is always possible.</p><h2 id="health-check" tabindex="-1">Health Check <a class="header-anchor" href="#health-check" aria-label="Permalink to &quot;Health Check&quot;">​</a></h2><p>The etcd main container and the etcd-backup-restore sidecar containers will be configured with livenessProbe and readinessProbe which will indicate the health of the containers and effectively the corresponding ETCD cluster member pod.</p><h3 id="backup-failure" tabindex="-1">Backup Failure <a class="header-anchor" href="#backup-failure" aria-label="Permalink to &quot;Backup Failure&quot;">​</a></h3><p>As described <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests">above</a> using <code>readinessProbe</code> failures based on latest backup failure is not viable in the multi-node ETCD scenario.</p><p>Though cutting off traffic by <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">manipulating client <code>service</code> <code>podSelector</code></a> is workable, it may not be desirable.</p><p>It is recommended that on backup failure, the leading <code>etcd-backup-restore</code> sidecar (the one that is responsible for taking backups at that point in time, as explained in the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">backup section below</a>, updates the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions"><code>BackupReady</code> condition</a> in the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status"><code>Etcd</code> status</a> and raises a high priority alert to the landscape operators but <em><em>does not</em></em> cut off the client traffic.</p><p>The reasoning behind this decision to not cut off the client traffic on backup failure is to allow the Kubernetes cluster&#39;s control plane (which relies on the ETCD cluster) to keep functioning as long as possible and to avoid bringing down the control-plane due to a missed backup.</p><p>The risk of this approach is that with a cascaded sequence of failures (on top of the backup failure), there is a chance of more data loss than the frequency of backup would otherwise indicate.</p><p>To be precise, the risk of such an additional data loss manifests only when backup failure as well as a special case of quorum loss (majority of the members are not ready) happen in such a way that the ETCD cluster needs to be re-bootstrapped from the backup. As described <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here</a>, re-bootstrapping the ETCD cluster requires restoration from the latest backup only when a majority of members no longer have uncorrupted data persistence.</p><p>If <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent storage</a> is used, this will happen only when backup failure as well as a majority of the disks/volumes backing the ETCD cluster members fail simultaneously. This would indeed be rare and might be an acceptable risk.</p><p>If <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral storage</a> is used (especially, <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">in-memory</a>), the data loss will happen if a majority of the ETCD cluster members become <code>NotReady</code> (requiring a pod restart) at the same time as the backup failure. This may not be as rare as majority members&#39; disk/volume failure. The risk can be somewhat mitigated at least for planned maintenance operations by postponing potentially disruptive maintenance operations when <code>BackupReady</code> condition is <code>false</code> (vertical scaling, rolling updates, evictions due to node roll-outs).</p><p>But in practice (when <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral storage</a> is used), the current proposal suggests restoring from the latest full backup even when a minority of ETCD members (even a single pod) <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restart</a> both to speed up the process of the new member catching up to the latest revision but also to avoid load on the leading member which needs to supply the data to bring the new member up-to-date. But as described <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">here</a>, in case of a minority member failure while using ephemeral storage, it is possible to restart the new member with empty data and let it fetch all the data from the leading member (only if backup is not accessible). Though this is suboptimal, it is workable given the constraints and conditions. With this, the risk of additional data loss in the case of ephemeral storage is only if backup failure as well as quorum loss happens. While this is still less rare than the risk of additional data loss in case of persistent storage, the risk might be tolerable. Provided the risk of quorum loss is not too high. This needs to be monitored/evaluated before opting for ephemeral storage.</p><p>Given these constraints, it is better to dynamically avoid/postpone some potentially disruptive operations when <code>BackupReady</code> condition is <code>false</code>. This has the effect of allowing <code>n/2</code> members to be evicted when the backups are healthy and completely disabling evictions when backups are not healthy.</p><ol><li>Skip/postpone potentially disruptive maintenance operations (listed below) when the <code>BackupReady</code> condition is <code>false</code>.</li><li>Vertical scaling.</li><li>Rolling updates, Basically, any updates to the <code>StatefulSet</code> spec which includes vertical scaling.</li><li>Dynamically toggle the <code>minAvailable</code> field of the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget"><code>PodDisruptionBudget</code></a> between <code>n/2 + 1</code> and <code>n</code> (where <code>n</code> is the ETCD desired cluster size) whenever the <code>BackupReady</code> condition toggles between <code>true</code> and <code>false</code>.</li></ol><p>This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there might be reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to update the <code>etcd</code> resource <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status</a> with latest full snapshot details). This enhancement should keep <code>etcd-backup-restore</code> backward compatible. I.e. it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-client-service-updates</code> which can be defaulted to <code>false</code> for backward compatibility).</p><h5 id="alternative-3" tabindex="-1">Alternative <a class="header-anchor" href="#alternative-3" aria-label="Permalink to &quot;Alternative&quot;">​</a></h5><p>The alternative is for <code>etcd-druid</code> to implement the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">above functionality</a>.</p><p>But <code>etcd-druid</code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally. So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages. Also, the recommended approach above is more robust because it can work even if <code>etcd-druid</code> is down when the backup upload of a particular etcd cluster fails.</p><h2 id="status" tabindex="-1">Status <a class="header-anchor" href="#status" aria-label="Permalink to &quot;Status&quot;">​</a></h2><p>It is desirable (for the <code>etcd-druid</code> and landscape administrators/operators) to maintain/expose status of the etcd cluster instances in the <code>status</code> sub-resource of the <code>Etcd</code> CRD. The proposed structure for maintaining the status is as shown in the example below.</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">druid.gardener.cloud/v1alpha1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Etcd</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-main</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  replicas</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">status</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  conditions</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Ready</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">                 # Condition type for the readiness of the ETCD cluster</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    status</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;True&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">              # Indicates of the ETCD Cluster is ready or not</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastHeartbeatTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:          </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastTransitionTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:         </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    reason</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Quorate</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">             # Quorate|QuorumLost</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">AllMembersReady</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">       # Condition type for the readiness of all the member of the ETCD cluster</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    status</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;True&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">              # Indicates if all the members of the ETCD Cluster are ready</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastHeartbeatTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:          </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastTransitionTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:         </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    reason</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">AllMembersReady</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">     # AllMembersReady|NotAllMembersReady</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">BackupReady</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">           # Condition type for the readiness of the backup of the ETCD cluster</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    status</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;True&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">              # Indicates if the backup of the ETCD cluster is ready</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastHeartbeatTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:          </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastTransitionTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:         </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    reason</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">FullBackupSucceeded</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # FullBackupSucceeded|IncrementalBackupSucceeded|FullBackupFailed|IncrementalBackupFailed</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  clusterSize</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  replicas</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  members</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-main-0</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # member pod name</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    id</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">272e204152</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">             # member Id</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    role</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Leader</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">               # Member|Leader</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    status</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Ready</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">              # Ready|NotReady|Unknown</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastTransitionTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:        </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    reason</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">LeaseSucceeded</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">     # LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">etcd-main-1</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # member pod name</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    id</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">272e204153</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">             # member Id</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    role</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Member</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">               # Member|Leader</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    status</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Ready</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">              # Ready|NotReady|Unknown</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    lastTransitionTime</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:        </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2020-11-10T12:48:01Z&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    reason</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">LeaseSucceeded</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">     # LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead</span></span></code></pre></div><p>This proposal recommends that <code>etcd-druid</code> (preferrably, the <code>custodian</code> controller in <code>etcd-druid</code>) maintains most of the information in the <code>status</code> of the <code>Etcd</code> resources described above.</p><p>One exception to this is the <code>BackupReady</code> condition which is recommended to be maintained by the <em>leading</em> <code>etcd-backup-restore</code> sidecar container. This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there are other reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">maintain health conditions</a>). This enhancement should keep <code>etcd-backup-restore</code> backward compatible. But it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-etcd-status-updates</code> which can be defaulted to <code>false</code> for backward compatibility).</p><h3 id="members" tabindex="-1">Members <a class="header-anchor" href="#members" aria-label="Permalink to &quot;Members&quot;">​</a></h3><p>The <code>members</code> section of the status is intended to be maintained by <code>etcd-druid</code> (preferraby, the <code>custodian</code> controller of <code>etcd-druid</code>) based on the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases"><code>leases</code> of the individual members</a>.</p><h4 id="note-1" tabindex="-1">Note <a class="header-anchor" href="#note-1" aria-label="Permalink to &quot;Note&quot;">​</a></h4><p>An earlier design in this proposal was for the individual <code>etcd-backup-restore</code> sidecars to update the corresponding <code>status.members</code> entries themselves. But this was redesigned to use <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member <code>leases</code></a> to avoid conflicts rising from frequent updates and the limitations in the support for <a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/" target="_blank" rel="noreferrer">Server-Side Apply</a> in some versions of Kubernetes.</p><p>The <code>spec.holderIdentity</code> field in the <code>leases</code> is used to communicate the ETCD member <code>id</code> and <code>role</code> between the <code>etcd-backup-restore</code> sidecars and <code>etcd-druid</code>.</p><h4 id="member-name-as-the-key" tabindex="-1">Member name as the key <a class="header-anchor" href="#member-name-as-the-key" aria-label="Permalink to &quot;Member name as the key&quot;">​</a></h4><p>In an ETCD cluster, the member <code>id</code> is the <a href="https://etcd.io/docs/v3.4/dev-guide/api_reference_v3/#message-member-etcdserveretcdserverpbrpcproto" target="_blank" rel="noreferrer">unique identifier for a member</a>. However, this proposal recommends using a <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">single <code>StatefulSet</code></a> whose pods form the members of the ETCD cluster and <code>Pods</code> of a <code>StatefulSet</code> have <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index" target="_blank" rel="noreferrer">uniquely indexed names</a> as well as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id" target="_blank" rel="noreferrer">uniquely addressible DNS</a>.</p><p>This proposal recommends that the <code>name</code> of the member (which is the same as the name of the member <code>Pod</code>) be used as the unique key to identify a member in the <code>members</code> array. This can minimise the need to cleanup superfluous entries in the <code>members</code> array after the member pods are gone to some extent because the replacement pods for any member will share the same <code>name</code> and will overwrite the entry with a <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">possibly new</a> member <code>id</code>.</p><p>There is still the possibility of not only <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in the <code>members</code> array</a> but also <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">superfluous <code>members</code> in the ETCD cluster</a> for which there is no corresponding pod in the <code>StatefulSet</code> anymore.</p><p>For example, if an ETCD cluster is scaled up from <code>3</code> to <code>5</code> and the new members were failing constantly due to insufficient resources and then if the ETCD client is scaled back down to <code>3</code> and failing member pods may not have the chance to clean up their <code>member</code> entries (from the <code>members</code> array as well as from the ETCD cluster) leading to superfluous members in the cluster that may have adverse effect on quorum of the cluster.</p><p>Hence, the superfluous entries in both <code>members</code> array as well as the ETCD cluster need to be <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">cleaned up</a> <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">as appropriate</a>.</p><h4 id="member-leases" tabindex="-1">Member Leases <a class="header-anchor" href="#member-leases" aria-label="Permalink to &quot;Member Leases&quot;">​</a></h4><p>One <a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/" target="_blank" rel="noreferrer">Kubernetes <code>lease</code> object</a> per desired ETCD member is maintained by <code>etcd-druid</code> (preferrably, the <code>custodian</code> controller in <code>etcd-druid</code>). The <code>lease</code> objects will be created in the same <code>namespace</code> as their owning <code>Etcd</code> object and will have the same <code>name</code> as the member to which they correspond (which, in turn would be the same as <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key">the <code>pod</code> name in which the member ETCD process runs</a>).</p><p>The <code>lease</code> objects are created and deleted only by <code>etcd-druid</code> but are continually renewed within the <code>leaseDurationSeconds</code> by the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members">individual <code>etcd-backup-restore</code> sidecars</a> (corresponding to their members) if the the corresponding ETCD member is ready and is part of the ETCD cluster.</p><p>This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there are other reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">maintain health conditions</a>). This enhancement should keep <code>etcd-backup-restore</code> backward compatible. But it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-etcd-lease-renewal</code> which can be defaulted to <code>false</code> for backward compatibility).</p><p>A <code>member</code> entry in the <code>Etcd</code> resource <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status"><code>status</code></a> would be marked as <code>Ready</code> (with <code>reason: LeaseSucceeded</code>) if the corresponding <code>pod</code> is ready and the corresponding <code>lease</code> has not yet expired. The <code>member</code> entry would be marked as <code>NotReady</code> if the corresponding <code>pod</code> is not ready (with reason <code>PodNotReady</code>) or as <code>Unknown</code> if the corresponding <code>lease</code> has expired (with <code>reason: LeaseExpired</code>).</p><p>While renewing the lease, the <code>etcd-backup-restore</code> sidecars also maintain the ETCD member <code>id</code> and their <code>role</code> (<code>Leader</code> or <code>Member</code>) separated by <code>:</code> in the <code>spec.holderIdentity</code> field of the corresponding <code>lease</code> object since this information is only available to the <code>ETCD</code> member processes and the <code>etcd-backup-restore</code> sidecars (e.g. <code>272e204152:Leader</code> or <code>272e204153:Member</code>). When the <code>lease</code> objects are created by <code>etcd-druid</code>, the <code>spec.holderIdentity</code> field would be empty.</p><p>The value in <code>spec.holderIdentity</code> in the <code>leases</code> is parsed and copied onto the <code>id</code> and <code>role</code> fields of the corresponding <code>status.members</code> by <code>etcd-druid</code>.</p><h3 id="conditions" tabindex="-1">Conditions <a class="header-anchor" href="#conditions" aria-label="Permalink to &quot;Conditions&quot;">​</a></h3><p>The <code>conditions</code> section in the status describe the overall condition of the ETCD cluster. The condition type <code>Ready</code> indicates if the ETCD cluster as a whole is ready to serve requests (i.e. the cluster is quorate) even though some minority of the members are not ready. The condition type <code>AllMembersReady</code> indicates of all the members of the ETCD cluster are ready. The distinction between these conditions could be significant for both external consumers of the status as well as <code>etcd-druid</code> itself. Some maintenance operations might be safe to do (e.g. rolling updates) only when all members of the cluster are ready. The condition type <code>BackupReady</code> indicates of the most recent backup upload (full or incremental) succeeded. This information also might be significant because some maintenance operations might be safe to do (e.g. anything that involves re-bootstrapping the ETCD cluster) only when backup is ready.</p><p>The <code>Ready</code> and <code>AllMembersReady</code> conditions can be maintained by <code>etcd-druid</code> based on the status in the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members"><code>members</code> section</a>. The <code>BackupReady</code> condition will be maintained by the leading <code>etcd-backup-restore</code> sidecar that is in charge of taking backups.</p><p>More condition types could be introduced in the future if specific purposes arise.</p><h3 id="clustersize" tabindex="-1">ClusterSize <a class="header-anchor" href="#clustersize" aria-label="Permalink to &quot;ClusterSize&quot;">​</a></h3><p>The <code>clusterSize</code> field contains the current size of the ETCD cluster. It will be actively kept up-to-date by <code>etcd-druid</code> in all scenarios.</p><ul><li>Before <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping</a> the ETCD cluster (during cluster creation or later bootstrapping because of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9">quorum failure</a>), <code>etcd-druid</code> will clear the <code>status.members</code> array and set <code>status.clusterSize</code> to be equal to <code>spec.replicas</code>.</li><li>While the ETCD cluster is quorate, <code>etcd-druid</code> will actively set <code>status.clusterSize</code> to be equal to length of the <code>status.members</code> whenever the length of the array changes (say, due to scaling of the ETCD cluster).</li></ul><p>Given that <code>clusterSize</code> reliably represents the size of the ETCD cluster, it can be used to calculate the <code>Ready</code> <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">condition</a>.</p><h3 id="alternative-4" tabindex="-1">Alternative <a class="header-anchor" href="#alternative-4" aria-label="Permalink to &quot;Alternative&quot;">​</a></h3><p>The alternative is for <code>etcd-druid</code> to maintain the status in the <code>Etcd</code> status sub-resource. But <code>etcd-druid</code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally. So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages. Also, the recommended approach above is more robust because it can work even if <code>etcd-druid</code> is down when the backup upload of a particular etcd cluster fails.</p><h2 id="decision-table-for-etcd-druid-based-on-the-status" tabindex="-1">Decision table for etcd-druid based on the status <a class="header-anchor" href="#decision-table-for-etcd-druid-based-on-the-status" aria-label="Permalink to &quot;Decision table for etcd-druid based on the status&quot;">​</a></h2><p>The following decision table describes the various criteria <code>etcd-druid</code> takes into consideration to determine the different etcd cluster management scenarios and the corresponding reconciliation actions it must take. The general principle is to detect the scenario and take the minimum action to move the cluster along the path to good health. The path from any one scenario to a state of good health will typically involve going through multiple reconciliation actions which probably take the cluster through many other cluster management scenarios. Especially, it is proposed that individual members auto-heal where possible, even in the case of the failure of a majority of members of the etcd cluster and that <code>etcd-druid</code> takes action only if the auto-healing doesn&#39;t happen for a configured period of time.</p><h3 id="_1-pink-of-health" tabindex="-1">1. Pink of health <a class="header-anchor" href="#_1-pink-of-health" aria-label="Permalink to &quot;1. Pink of health&quot;">​</a></h3><h4 id="observed-state" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: <code>n</code></li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>0</code></li></ul></li><li>conditions: <ul><li>Ready: <code>true</code></li><li>AllMembersReady: <code>true</code></li><li>BackupReady: <code>true</code></li></ul></li></ul></li></ul><h4 id="recommended-action" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Nothing to do</p><h3 id="_2-member-status-is-out-of-sync-with-their-leases" tabindex="-1">2. Member status is out of sync with their leases <a class="header-anchor" href="#_2-member-status-is-out-of-sync-with-their-leases" aria-label="Permalink to &quot;2. Member status is out of sync with their leases&quot;">​</a></h3><h4 id="observed-state-1" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-1" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: <code>n</code></li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>r</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>l</code></li></ul></li><li>conditions: <ul><li>Ready: <code>true</code></li><li>AllMembersReady: <code>true</code></li><li>BackupReady: <code>true</code></li></ul></li></ul></li></ul><h4 id="recommended-action-1" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-1" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Mark the <code>l</code> members corresponding to the expired <code>leases</code> as <code>Unknown</code> with reason <code>LeaseExpired</code> and with <code>id</code> populated from <code>spec.holderIdentity</code> of the <code>lease</code> if they are not already updated so.</p><p>Mark the <code>n - l</code> members corresponding to the active <code>leases</code> as <code>Ready</code> with reason <code>LeaseSucceeded</code> and with <code>id</code> populated from <code>spec.holderIdentity</code> of the <code>lease</code> if they are not already updated so.</p><p>Please refer <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">here</a> for more details.</p><h3 id="_3-all-members-are-ready-but-allmembersready-condition-is-stale" tabindex="-1">3. All members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale <a class="header-anchor" href="#_3-all-members-are-ready-but-allmembersready-condition-is-stale" aria-label="Permalink to &quot;3. All members are \`Ready\` but \`AllMembersReady\` condition is stale&quot;">​</a></h3><h4 id="observed-state-2" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-2" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>0</code></li></ul></li><li>conditions: <ul><li>Ready: N/A</li><li>AllMembersReady: false</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id="recommended-action-2" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-2" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Mark the status condition type <code>AllMembersReady</code> to <code>true</code>.</p><h3 id="_4-not-all-members-are-ready-but-allmembersready-condition-is-stale" tabindex="-1">4. Not all members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale <a class="header-anchor" href="#_4-not-all-members-are-ready-but-allmembersready-condition-is-stale" aria-label="Permalink to &quot;4. Not all members are \`Ready\` but \`AllMembersReady\` condition is stale&quot;">​</a></h3><h4 id="observed-state-3" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-3" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members <ul><li>Total: N/A</li><li>Ready: <code>r</code> where <code>0 &lt;= r &lt; n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n</code></li><li>Members with expired <code>lease</code>: <code>h</code> where <code>0 &lt; h &lt; n</code></li></ul></li><li>conditions: <ul><li>Ready: N/A</li><li>AllMembersReady: true</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>(nr + u + h) &gt; 0</code> or <code>r &lt; n</code></p></li></ul><h4 id="recommended-action-3" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-3" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Mark the status condition type <code>AllMembersReady</code> to <code>false</code>.</p><h3 id="_5-majority-members-are-ready-but-ready-condition-is-stale" tabindex="-1">5. Majority members are <code>Ready</code> but <code>Ready</code> condition is stale <a class="header-anchor" href="#_5-majority-members-are-ready-but-ready-condition-is-stale" aria-label="Permalink to &quot;5. Majority members are \`Ready\` but \`Ready\` condition is stale&quot;">​</a></h3><h4 id="observed-state-4" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-4" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>r</code> where <code>r &gt; n/2</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n/2</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions: <ul><li>Ready: <code>false</code></li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>0 &lt; (nr + u + h) &lt; n/2</code></p></li></ul><h4 id="recommended-action-4" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-4" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Mark the status condition type <code>Ready</code> to <code>true</code>.</p><h3 id="_6-majority-members-are-notready-but-ready-condition-is-stale" tabindex="-1">6. Majority members are <code>NotReady</code> but <code>Ready</code> condition is stale <a class="header-anchor" href="#_6-majority-members-are-notready-but-ready-condition-is-stale" aria-label="Permalink to &quot;6. Majority members are \`NotReady\` but \`Ready\` condition is stale&quot;">​</a></h3><h4 id="observed-state-5" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-5" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>r</code> where <code>0 &lt; r &lt; n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions: <ul><li>Ready: <code>true</code></li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>(nr + u + h) &gt; n/2</code> or <code>r &lt; n/2</code></p></li></ul><h4 id="recommended-action-5" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-5" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Mark the status condition type <code>Ready</code> to <code>false</code>.</p><h3 id="_7-some-members-have-been-in-unknown-status-for-a-while" tabindex="-1">7. Some members have been in <code>Unknown</code> status for a while <a class="header-anchor" href="#_7-some-members-have-been-in-unknown-status-for-a-while" aria-label="Permalink to &quot;7. Some members have been in \`Unknown\` status for a while&quot;">​</a></h3><h4 id="observed-state-6" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-6" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: N/A</li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>u</code> where <code>u &lt;= n</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions: <ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id="recommended-action-6" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-6" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Mark the <code>u</code> members as <code>NotReady</code> in <code>Etcd</code> status with <code>reason: UnknownGracePeriodExceeded</code>.</p><h3 id="_8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status" tabindex="-1">8. Some member pods are not <code>Ready</code> but have not had the chance to update their status <a class="header-anchor" href="#_8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status" aria-label="Permalink to &quot;8. Some member pods are not \`Ready\` but have not had the chance to update their status&quot;">​</a></h3><h4 id="observed-state-7" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-7" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: <code>n</code></li><li>Ready: <code>s</code> where <code>s &lt; n</code></li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: N/A</li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions: <ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id="recommended-action-7" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-7" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Mark the <code>n - s</code> members (corresponding to the pods that are not <code>Ready</code>) as <code>NotReady</code> in <code>Etcd</code> status with <code>reason: PodNotReady</code></p><h3 id="_9-quorate-cluster-with-a-minority-of-members-notready" tabindex="-1">9. Quorate cluster with a minority of members <code>NotReady</code> <a class="header-anchor" href="#_9-quorate-cluster-with-a-minority-of-members-notready" aria-label="Permalink to &quot;9. Quorate cluster with a minority of members \`NotReady\`&quot;">​</a></h3><h4 id="observed-state-8" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-8" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>n - f</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>f</code> where <code>f &lt; n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions: <ul><li>Ready: true</li><li>AllMembersReady: false</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id="recommended-action-8" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-8" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Delete the <code>f</code> <code>NotReady</code> member pods to force restart of the pods if they do not automatically restart via failed <code>livenessProbe</code>. The expectation is that they will either re-join the cluster as an existing member or remove themselves and join as new members on restart of the container or pod and <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">renew their <code>leases</code></a>.</p><h3 id="_10-quorum-lost-with-a-majority-of-members-notready" tabindex="-1">10. Quorum lost with a majority of members <code>NotReady</code> <a class="header-anchor" href="#_10-quorum-lost-with-a-majority-of-members-notready" aria-label="Permalink to &quot;10. Quorum lost with a majority of members \`NotReady\`&quot;">​</a></h3><h4 id="observed-state-9" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-9" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>n - f</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: <code>f</code> where <code>f &gt;= n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions: <ul><li>Ready: false</li><li>AllMembersReady: false</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id="recommended-action-9" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-9" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Scale down the <code>StatefulSet</code> to <code>replicas: 0</code>. Ensure that all member pods are deleted. Ensure that all the members are removed from <code>Etcd</code> status. Delete and recreate all the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member <code>leases</code></a>. Recover the cluster from loss of quorum as discussed <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here</a>.</p><h3 id="_11-scale-up-of-a-healthy-cluster" tabindex="-1">11. Scale up of a healthy cluster <a class="header-anchor" href="#_11-scale-up-of-a-healthy-cluster" aria-label="Permalink to &quot;11. Scale up of a healthy cluster&quot;">​</a></h3><h4 id="observed-state-10" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-10" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: <code>d</code></li><li>Current: <code>n</code> where <code>d &gt; n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: N/A</li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: 0</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: 0</li><li>Members with expired <code>lease</code>: 0</li></ul></li><li>conditions: <ul><li>Ready: true</li><li>AllMembersReady: true</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id="recommended-action-10" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-10" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Add <code>d - n</code> new members by scaling the <code>StatefulSet</code> to <code>replicas: d</code>. The rest of the <code>StatefulSet</code> spec need not be updated until the next cluster bootstrapping (alternatively, the rest of the <code>StatefulSet</code> spec can be updated pro-actively once the new members join the cluster. This will trigger a rolling update).</p><p>Also, create the additional <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member <code>leases</code></a> for the <code>d - n</code> new members.</p><h3 id="_12-scale-down-of-a-healthy-cluster" tabindex="-1">12. Scale down of a healthy cluster <a class="header-anchor" href="#_12-scale-down-of-a-healthy-cluster" aria-label="Permalink to &quot;12. Scale down of a healthy cluster&quot;">​</a></h3><h4 id="observed-state-11" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-11" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: <code>d</code></li><li>Current: <code>n</code> where <code>d &lt; n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: 0</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: 0</li><li>Members with expired <code>lease</code>: 0</li></ul></li><li>conditions: <ul><li>Ready: true</li><li>AllMembersReady: true</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id="recommended-action-11" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-11" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Remove <code>d - n</code> existing members (numbered <code>d</code>, <code>d + 1</code> ... <code>n</code>) by scaling the <code>StatefulSet</code> to <code>replicas: d</code>. The <code>StatefulSet</code> spec need not be updated until the next cluster bootstrapping (alternatively, the <code>StatefulSet</code> spec can be updated pro-actively once the superfluous members exit the cluster. This will trigger a rolling update).</p><p>Also, delete the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member <code>leases</code></a> for the <code>d - n</code> members being removed.</p><p>The <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in the <code>members</code> array</a> will be cleaned up as explained <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">here</a>. The superfluous members in the ETCD cluster will be cleaned up by the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">leading <code>etcd-backup-restore</code> sidecar</a>.</p><h3 id="_13-superfluous-member-entries-in-etcd-status" tabindex="-1">13. Superfluous member entries in <code>Etcd</code> status <a class="header-anchor" href="#_13-superfluous-member-entries-in-etcd-status" aria-label="Permalink to &quot;13. Superfluous member entries in \`Etcd\` status&quot;">​</a></h3><h4 id="observed-state-12" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-12" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size <ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas <ul><li>Desired: n</li><li>Ready: n</li></ul></li><li><code>Etcd</code> status <ul><li>members <ul><li>Total: <code>m</code> where <code>m &gt; n</code></li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime &gt; notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime &gt; unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions: <ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id="recommended-action-12" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-12" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Remove the superfluous <code>m - n</code> member entries from <code>Etcd</code> status (numbered <code>n</code>, <code>n+1</code> ... <code>m</code>). Remove the superfluous <code>m - n</code> <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member <code>leases</code></a> if they exist. The superfluous members in the ETCD cluster will be cleaned up by the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">leading <code>etcd-backup-restore</code> sidecar</a>.</p><h2 id="decision-table-for-etcd-backup-restore-during-initialization" tabindex="-1">Decision table for etcd-backup-restore during initialization <a class="header-anchor" href="#decision-table-for-etcd-backup-restore-during-initialization" aria-label="Permalink to &quot;Decision table for etcd-backup-restore during initialization&quot;">​</a></h2><p>As discussed above, the initialization sequence of <code>etcd-backup-restore</code> in a member pod needs to <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">generate suitable etcd configuration</a> for its etcd container. It also might have to handle the etcd database verification and restoration functionality differently in <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">different</a> <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">scenarios</a>.</p><p>The initialization sequence itself is proposed to be as follows. It is an enhancement of the <a href="https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/design.md#workflow" target="_blank" rel="noreferrer">existing</a> initialization sequence. <img src="`+r+'" alt="etcd member initialization sequence"></p><p>The details of the decisions to be taken during the initialization are given below.</p><h3 id="_1-first-member-during-bootstrap-of-a-fresh-etcd-cluster" tabindex="-1">1. First member during bootstrap of a fresh etcd cluster <a class="header-anchor" href="#_1-first-member-during-bootstrap-of-a-fresh-etcd-cluster" aria-label="Permalink to &quot;1. First member during bootstrap of a fresh etcd cluster&quot;">​</a></h3><h4 id="observed-state-13" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-13" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>0</code></li><li>Ready: <code>0</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup <ul><li>Backup exists: <code>false</code></li><li>Backup has incremental snapshots: <code>false</code></li></ul></li></ul><h4 id="recommended-action-13" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-13" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state new and return success.</p><h3 id="_2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster" tabindex="-1">2. Addition of a new following member during bootstrap of a fresh etcd cluster <a class="header-anchor" href="#_2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster" aria-label="Permalink to &quot;2. Addition of a new following member during bootstrap of a fresh etcd cluster&quot;">​</a></h3><h4 id="observed-state-14" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-14" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>m</code> where <code>0 &lt; m &lt; n</code></li><li>Ready: <code>m</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup <ul><li>Backup exists: <code>false</code></li><li>Backup has incremental snapshots: <code>false</code></li></ul></li></ul><h4 id="recommended-action-14" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-14" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state new and return success.</p><h3 id="_3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data" tabindex="-1">3. Restart of an existing member of a quorate cluster with valid metadata and data <a class="header-anchor" href="#_3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data" aria-label="Permalink to &quot;3. Restart of an existing member of a quorate cluster with valid metadata and data&quot;">​</a></h3><h4 id="observed-state-15" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-15" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>m</code> where <code>m &gt; n/2</code></li><li>Ready: <code>r</code> where <code>r &gt; n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>true</code></li></ul></li><li>Backup <ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id="recommended-action-15" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-15" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Re-use previously generated etcd configuration and return success.</p><h3 id="_4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data" tabindex="-1">4. Restart of an existing member of a quorate cluster with valid metadata but without valid data <a class="header-anchor" href="#_4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data" aria-label="Permalink to &quot;4. Restart of an existing member of a quorate cluster with valid metadata but without valid data&quot;">​</a></h3><h4 id="observed-state-16" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-16" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>m</code> where <code>m &gt; n/2</code></li><li>Ready: <code>r</code> where <code>r &gt; n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup <ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id="recommended-action-16" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-16" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p><a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Remove</a> self as a member (old member ID) from the etcd cluster as well as <code>Etcd</code> status. <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add</a> self as a new member of the etcd cluster as well as in the <code>Etcd</code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h3 id="_5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata" tabindex="-1">5. Restart of an existing member of a quorate cluster without valid metadata <a class="header-anchor" href="#_5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata" aria-label="Permalink to &quot;5. Restart of an existing member of a quorate cluster without valid metadata&quot;">​</a></h3><h4 id="observed-state-17" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-17" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>m</code> where <code>m &gt; n/2</code></li><li>Ready: <code>r</code> where <code>r &gt; n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: N/A</li></ul></li><li>Backup <ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id="recommended-action-17" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-17" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p><a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Remove</a> self as a member (old member ID) from the etcd cluster as well as <code>Etcd</code> status. <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add</a> self as a new member of the etcd cluster as well as in the <code>Etcd</code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h3 id="_6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data" tabindex="-1">6. Restart of an existing member of a non-quorate cluster with valid metadata and data <a class="header-anchor" href="#_6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data" aria-label="Permalink to &quot;6. Restart of an existing member of a non-quorate cluster with valid metadata and data&quot;">​</a></h3><h4 id="observed-state-18" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-18" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>m</code> where <code>m &lt; n/2</code></li><li>Ready: <code>r</code> where <code>r &lt; n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>true</code></li></ul></li><li>Backup <ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id="recommended-action-18" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-18" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>Re-use previously generated etcd configuration and return success.</p><h3 id="_7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data" tabindex="-1">7. Restart of the first member of a non-quorate cluster without valid data <a class="header-anchor" href="#_7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data" aria-label="Permalink to &quot;7. Restart of the first member of a non-quorate cluster without valid data&quot;">​</a></h3><h4 id="observed-state-19" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-19" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>0</code></li><li>Ready: <code>0</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: N/A</li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup <ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id="recommended-action-19" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-19" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore the latest full snapshot. Start a single-node embedded etcd with initial cluster peer URLs containing only own peer URL and initial cluster state <code>new</code>. If incremental snapshots exist, apply them serially (honouring source transactions). Take and upload a full snapshot after incremental snapshots are applied successfully (please see <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here</a> for more reasons why). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>new</code> and return success.</p><h3 id="_8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data" tabindex="-1">8. Restart of a following member of a non-quorate cluster without valid data <a class="header-anchor" href="#_8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data" aria-label="Permalink to &quot;8. Restart of a following member of a non-quorate cluster without valid data&quot;">​</a></h3><h4 id="observed-state-20" tabindex="-1">Observed state <a class="header-anchor" href="#observed-state-20" aria-label="Permalink to &quot;Observed state&quot;">​</a></h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members: <ul><li>Total: <code>m</code> where <code>1 &lt; m &lt; n</code></li><li>Ready: <code>r</code> where <code>1 &lt; r &lt; n</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence <ul><li>WAL directory has cluster/ member metadata: N/A</li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup <ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id="recommended-action-20" tabindex="-1">Recommended Action <a class="header-anchor" href="#recommended-action-20" aria-label="Permalink to &quot;Recommended Action&quot;">​</a></h4><p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h2 id="backup" tabindex="-1">Backup <a class="header-anchor" href="#backup" aria-label="Permalink to &quot;Backup&quot;">​</a></h2><p>Only one of the etcd-backup-restore sidecars among the members are required to take the backup for a given ETCD cluster. This can be called a <code>backup leader</code>. There are two possibilities to ensure this.</p><h3 id="leading-etcd-main-container-s-sidecar-is-the-backup-leader" tabindex="-1">Leading ETCD main container’s sidecar is the backup leader <a class="header-anchor" href="#leading-etcd-main-container-s-sidecar-is-the-backup-leader" aria-label="Permalink to &quot;Leading ETCD main container’s sidecar is the backup leader&quot;">​</a></h3><p>The backup-restore sidecar could poll the etcd cluster and/or its own etcd main container to see if it is the leading member in the etcd cluster. This information can be used by the backup-restore sidecars to decide that sidecar of the leading etcd main container is the backup leader (i.e. responsible to for taking/uploading backups regularly).</p><p>The advantages of this approach are as follows.</p><ul><li>The approach is operationally and conceptually simple. The leading etcd container and backup-restore sidecar are always located in the same pod.</li><li>Network traffic between the backup container and the etcd cluster will always be local.</li></ul><p>The disadvantage is that this approach may not age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.</p><h3 id="independent-leader-election-between-backup-restore-sidecars" tabindex="-1">Independent leader election between backup-restore sidecars <a class="header-anchor" href="#independent-leader-election-between-backup-restore-sidecars" aria-label="Permalink to &quot;Independent leader election between backup-restore sidecars&quot;">​</a></h3><p>We could use the etcd <code>lease</code> mechanism to perform leader election among the backup-restore sidecars. For example, using something like <a href="https://pkg.go.dev/go.etcd.io/etcd/clientv3/concurrency#Election.Campaign" target="_blank" rel="noreferrer"><code>go.etcd.io/etcd/clientv3/concurrency</code></a>.</p><p>The advantage and disadvantages are pretty much the opposite of the approach <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader">above</a>. The advantage being that this approach may age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.</p><p>The disadvantages are as follows.</p><ul><li>The approach is operationally and conceptually a bit complex. The leading etcd container and backup-restore sidecar might potentially belong to different pods.</li><li>Network traffic between the backup container and the etcd cluster might potentially be across nodes.</li></ul><h2 id="history-compaction" tabindex="-1">History Compaction <a class="header-anchor" href="#history-compaction" aria-label="Permalink to &quot;History Compaction&quot;">​</a></h2><p>This proposal recommends to configure <a href="https://etcd.io/docs/v3.2.17/op-guide/maintenance/#history-compaction" target="_blank" rel="noreferrer">automatic history compaction</a> on the individual members.</p><h2 id="defragmentation" tabindex="-1">Defragmentation <a class="header-anchor" href="#defragmentation" aria-label="Permalink to &quot;Defragmentation&quot;">​</a></h2><p>Defragmentation is already <a href="https://github.com/gardener/etcd-backup-restore/blob/0dfdd50fbfc5ebc88238be3bc79c3ac3fc242c08/cmd/options.go#L209" target="_blank" rel="noreferrer">triggered periodically</a> by <code>etcd-backup-restore</code>. This proposal recommends to enhance this functionality to be performed only by the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">leading</a> backup-restore container. The defragmentation must be performed only when etcd cluster is in full health and must be done in a rolling manner for each members to <a href="https://etcd.io/docs/v3.2.17/op-guide/maintenance/#defragmentation" target="_blank" rel="noreferrer">avoid disruption</a>. The leading member should be defragmented last after all the rest of the members have been defragmented to minimise potential leadership changes caused by defragmentation. If the etcd cluster is unhealthy when it is time to trigger scheduled defragmentation, the defragmentation must be postponed until the cluster becomes healthy. This check must be done before triggering defragmentation for each member.</p><h2 id="work-flows-in-etcd-backup-restore" tabindex="-1">Work-flows in etcd-backup-restore <a class="header-anchor" href="#work-flows-in-etcd-backup-restore" aria-label="Permalink to &quot;Work-flows in etcd-backup-restore&quot;">​</a></h2><p>There are different work-flows in etcd-backup-restore. Some existing flows like initialization, scheduled backups and defragmentation have been enhanced or modified. Some new work-flows like status updates have been introduced. Some of these work-flows are sensitive to which <code>etcd-backup-restore</code> container is <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">leading</a> and some are not.</p><p>The life-cycle of these work-flows is shown below. <img src="'+n+`" alt="etcd-backup-restore work-flows life-cycle"></p><h3 id="work-flows-independent-of-leader-election-in-all-members" tabindex="-1">Work-flows independent of leader election in all members <a class="header-anchor" href="#work-flows-independent-of-leader-election-in-all-members" aria-label="Permalink to &quot;Work-flows independent of leader election in all members&quot;">​</a></h3><ul><li>Serve the <a href="https://github.com/gardener/etcd-backup-restore/blob/master/pkg/server/httpAPI.go#L101-L107" target="_blank" rel="noreferrer">HTTP API</a> that all members are expected to support currently but some HTTP API call which are used to take <a href="https://github.com/gardener/etcd-backup-restore/blob/5dfcc1f848a9f325d41a24eae4defb70d997c215/pkg/server/httpAPI.go#L103-L105" target="_blank" rel="noreferrer">out-of-sync delta or full snapshot</a> should delegate the incoming HTTP requests to the <code>leading-sidecar</code> and one of the possible approach to achieve this is via an <a href="https://pkg.go.dev/net/http/httputil#ReverseProxy.ServeHTTP" target="_blank" rel="noreferrer">HTTP reverse proxy</a>.</li><li>Check the health of the respective etcd member and renew the corresponding <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member <code>lease</code></a>.</li></ul><h3 id="work-flows-only-on-the-leading-member" tabindex="-1">Work-flows only on the leading member <a class="header-anchor" href="#work-flows-only-on-the-leading-member" aria-label="Permalink to &quot;Work-flows only on the leading member&quot;">​</a></h3><ul><li>Take <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">backups</a> (full and incremental) at configured regular intervals</li><li><a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation">Defragment</a> all the members sequentially at configured regular intervals</li><li>Cleanup superflous members from the ETCD cluster for which there is no corresponding pod (the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index" target="_blank" rel="noreferrer">ordinal</a> in the pod name is greater than the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize">cluster size</a>) at regular intervals (or whenever the <code>Etcd</code> resource <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status</a> changes by watching it) <ul><li>The cleanup of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in <code>status.members</code> array</a> is already covered <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">here</a></li></ul></li></ul><h2 id="high-availability" tabindex="-1">High Availability <a class="header-anchor" href="#high-availability" aria-label="Permalink to &quot;High Availability&quot;">​</a></h2><p>Considering that high-availability is the primary reason for using a multi-node etcd cluster, it makes sense to distribute the individual member pods of the etcd cluster across different physical nodes. If the underlying Kubernetes cluster has nodes from multiple availability zones, it makes sense to also distribute the member pods across nodes from different availability zones.</p><p>One possibility to do this is via <a href="https://kubernetes.io/docs/reference/scheduling/policies/#priorities" target="_blank" rel="noreferrer"><code>SelectorSpreadPriority</code></a> of <code>kube-scheduler</code> but this is only <a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone" target="_blank" rel="noreferrer">best-effort</a> and may not always be enforced strictly.</p><p>It is better to use <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity" target="_blank" rel="noreferrer">pod anti-affinity</a> to enforce such distribution of member pods.</p><h3 id="zonal-cluster-single-availability-zone" tabindex="-1">Zonal Cluster - Single Availability Zone <a class="header-anchor" href="#zonal-cluster-single-availability-zone" aria-label="Permalink to &quot;Zonal Cluster - Single Availability Zone&quot;">​</a></h3><p>A zonal cluster is configured to consist of nodes belonging to only a single availability zone in a region of the cloud provider. In such a case, we can at best distribute the member pods of a multi-node etcd cluster instance only across different nodes in the configured availability zone.</p><p>This can be done by specifying <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity" target="_blank" rel="noreferrer">pod anti-affinity</a> in the specification of the member pods using <a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-hostname" target="_blank" rel="noreferrer"><code>kubernetes.io/hostname</code></a> as the topology key.</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">apps/v1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">StatefulSet</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  template</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      affinity</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        podAntiAffinity</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          requiredDuringSchedulingIgnoredDuringExecution</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">labelSelector</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {} </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># podSelector that matches the member pods of the given etcd cluster instance</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">            topologyKey</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;kubernetes.io/hostname&quot;</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      ...</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    ...</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span></code></pre></div><p>The recommendation is to keep <code>etcd-druid</code> agnostic of such topics related scheduling and cluster-topology and to use <a href="https://github.com/gardener/kupid" target="_blank" rel="noreferrer">kupid</a> to <a href="https://github.com/gardener/kupid#mutating-higher-order-controllers" target="_blank" rel="noreferrer">orthogonally inject</a> the desired <a href="https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml" target="_blank" rel="noreferrer">pod anti-affinity</a>.</p><h4 id="alternative-5" tabindex="-1">Alternative <a class="header-anchor" href="#alternative-5" aria-label="Permalink to &quot;Alternative&quot;">​</a></h4><p>Another option is to build the functionality into <code>etcd-druid</code> to include the required pod anti-affinity when it provisions the <code>StatefulSet</code> that manages the member pods. While this has the advantage of avoiding a dependency on an external component like <a href="https://github.com/gardener/kupid" target="_blank" rel="noreferrer">kupid</a>, the disadvantage is that we might need to address development or testing use-cases where it might be desirable to avoid distributing member pods and schedule them on as less number of nodes as possible. Also, as mentioned <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones">below</a>, <a href="https://github.com/gardener/kupid" target="_blank" rel="noreferrer">kupid</a> can be used to distribute member pods of an etcd cluster instance across nodes in a single availability zone as well as across nodes in multiple availability zones with very minor variation. This keeps the solution uniform regardless of the topology of the underlying Kubernetes cluster.</p><h3 id="regional-cluster-multiple-availability-zones" tabindex="-1">Regional Cluster - Multiple Availability Zones <a class="header-anchor" href="#regional-cluster-multiple-availability-zones" aria-label="Permalink to &quot;Regional Cluster - Multiple Availability Zones&quot;">​</a></h3><p>A regional cluster is configured to consist of nodes belonging to multiple availability zones (typically, three) in a region of the cloud provider. In such a case, we can distribute the member pods of a multi-node etcd cluster instance across nodes belonging to different availability zones.</p><p>This can be done by specifying <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity" target="_blank" rel="noreferrer">pod anti-affinity</a> in the specification of the member pods using <a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone" target="_blank" rel="noreferrer"><code>topology.kubernetes.io/zone</code></a> as the topology key. In Kubernetes clusters using Kubernetes release older than <code>1.17</code>, the older (and now deprecated) <a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone" target="_blank" rel="noreferrer"><code>failure-domain.beta.kubernetes.io/zone</code></a> might have to be used as the topology key.</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">apps/v1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">StatefulSet</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  template</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      ...</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">      affinity</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        podAntiAffinity</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          requiredDuringSchedulingIgnoredDuringExecution</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">labelSelector</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {} </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># podSelector that matches the member pods of the given etcd cluster instance</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">            topologyKey</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;topology.kubernetes.io/zone</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">      ...</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    ...</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  ...</span></span></code></pre></div><p>The recommendation is to keep <code>etcd-druid</code> agnostic of such topics related scheduling and cluster-topology and to use <a href="https://github.com/gardener/kupid" target="_blank" rel="noreferrer">kupid</a> to <a href="https://github.com/gardener/kupid#mutating-higher-order-controllers" target="_blank" rel="noreferrer">orthogonally inject</a> the desired <a href="https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml" target="_blank" rel="noreferrer">pod anti-affinity</a>.</p><h4 id="alternative-6" tabindex="-1">Alternative <a class="header-anchor" href="#alternative-6" aria-label="Permalink to &quot;Alternative&quot;">​</a></h4><p>Another option is to build the functionality into <code>etcd-druid</code> to include the required pod anti-affinity when it provisions the <code>StatefulSet</code> that manages the member pods. While this has the advantage of avoiding a dependency on an external component like <a href="https://github.com/gardener/kupid" target="_blank" rel="noreferrer">kupid</a>, the disadvantage is that such built-in support necessarily limits what kind of topologies of the underlying cluster will be supported. Hence, it is better to keep <code>etcd-druid</code> altogether agnostic of issues related to scheduling and cluster-topology.</p><h3 id="poddisruptionbudget" tabindex="-1">PodDisruptionBudget <a class="header-anchor" href="#poddisruptionbudget" aria-label="Permalink to &quot;PodDisruptionBudget&quot;">​</a></h3><p>This proposal recommends that <code>etcd-druid</code> should deploy <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets" target="_blank" rel="noreferrer"><code>PodDisruptionBudget</code></a> (<code>minAvailable</code> set to <code>floor(&lt;cluster size&gt;/2) + 1</code>) for multi-node etcd clusters (if <code>AllMembersReady</code> <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">condition</a> is <code>true</code>) to ensure that any planned disruptive operation can try and honour the disruption budget to ensure high availability of the etcd cluster while making potentially disrupting maintenance operations.</p><p>Also, it is recommended to toggle the <code>minAvailable</code> field between <code>floor(&lt;cluster size&gt;/2)</code> and <code>&lt;number of members with status Ready true&gt;</code> whenever the <code>AllMembersReady</code> condition toggles between <code>true</code> and <code>false</code>. This is to disable eviction of any member pods when not all members are <code>Ready</code>.</p><p>In case of a conflict, the recommendation is to use the highest of the applicable values for <code>minAvailable</code>.</p><h2 id="rolling-updates-to-etcd-members" tabindex="-1">Rolling updates to etcd members <a class="header-anchor" href="#rolling-updates-to-etcd-members" aria-label="Permalink to &quot;Rolling updates to etcd members&quot;">​</a></h2><p>Any changes to the <code>Etcd</code> resource spec that might result in a change to <code>StatefulSet</code> spec or otherwise result in a rolling update of member pods should be applied/propagated by <code>etcd-druid</code> only when the etcd cluster is fully healthy to reduce the risk of quorum loss during the updates. This would include vertical autoscaling changes (via, <a href="https://github.com/gardener/hvpa-controller" target="_blank" rel="noreferrer">HVPA</a>). If the cluster <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status</a> unhealthy (i.e. if either <code>AllMembersReady</code> or <code>BackupReady</code> <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">conditions</a> are <code>false</code>), <code>etcd-druid</code> must restore it to full health <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">before proceeding</a> with such operations that lead to rolling updates. This can be further optimized in the future to handle the cases where rolling updates can still be performed on an etcd cluster that is not fully healthy.</p><h2 id="follow-up" tabindex="-1">Follow Up <a class="header-anchor" href="#follow-up" aria-label="Permalink to &quot;Follow Up&quot;">​</a></h2><h3 id="ephemeral-volumes" tabindex="-1">Ephemeral Volumes <a class="header-anchor" href="#ephemeral-volumes" aria-label="Permalink to &quot;Ephemeral Volumes&quot;">​</a></h3><p>See section <em><a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#Ephemeral_Volumes">Ephemeral Volumes</a></em>.</p><h3 id="shoot-control-plane-migration" tabindex="-1">Shoot Control-Plane Migration <a class="header-anchor" href="#shoot-control-plane-migration" aria-label="Permalink to &quot;Shoot Control-Plane Migration&quot;">​</a></h3><p>This proposal adds support for multi-node etcd clusters but it should not have significant impact on <a href="https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md" target="_blank" rel="noreferrer">shoot control-plane migration</a> any more than what already present in the single-node etcd cluster scenario. But to be sure, this needs to be discussed further.</p><h3 id="performance-impact-of-multi-node-etcd-clusters" tabindex="-1">Performance impact of multi-node etcd clusters <a class="header-anchor" href="#performance-impact-of-multi-node-etcd-clusters" aria-label="Permalink to &quot;Performance impact of multi-node etcd clusters&quot;">​</a></h3><p>Multi-node etcd clusters incur a cost on <a href="https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size" target="_blank" rel="noreferrer">write performance</a> as compared to single-node etcd clusters. This performance impact needs to be measured and documented. Here, we should compare different persistence option for the multi-nodeetcd clusters so that we have all the information necessary to take the decision balancing the high-availability, performance and costs.</p><h3 id="metrics-dashboards-and-alerts" tabindex="-1">Metrics, Dashboards and Alerts <a class="header-anchor" href="#metrics-dashboards-and-alerts" aria-label="Permalink to &quot;Metrics, Dashboards and Alerts&quot;">​</a></h3><p>There are already metrics exported by etcd and <code>etcd-backup-restore</code> which are visualized in monitoring dashboards and also used in triggering alerts. These might have hidden assumptions about single-node etcd clusters. These might need to be enhanced and potentially new metrics, dashboards and alerts configured to cover the multi-node etcd cluster scenario.</p><p>Especially, a high priority alert must be raised if <code>BackupReady</code> <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#condition">condition</a> becomes <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure"><code>false</code></a>.</p><h3 id="costs" tabindex="-1">Costs <a class="header-anchor" href="#costs" aria-label="Permalink to &quot;Costs&quot;">​</a></h3><p>Multi-node etcd clusters will clearly involve higher cost (when compared with single-node etcd clusters) just going by the CPU and memory usage for the additional members. Also, the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence">different options</a> for persistence for etcd data for the members will have different cost implications. Such cost impact needs to be assessed and documented to help navigate the trade offs between high availability, performance and costs.</p><h2 id="future-work" tabindex="-1">Future Work <a class="header-anchor" href="#future-work" aria-label="Permalink to &quot;Future Work&quot;">​</a></h2><h3 id="gardener-ring" tabindex="-1">Gardener Ring <a class="header-anchor" href="#gardener-ring" aria-label="Permalink to &quot;Gardener Ring&quot;">​</a></h3><p><a href="https://github.com/gardener/gardener/issues/233" target="_blank" rel="noreferrer">Gardener Ring</a>, requires provisioning and management of an etcd cluster with the members distributed across more than one Kubernetes cluster. This cannot be achieved by etcd-druid alone which has only the view of a single Kubernetes cluster. An additional component that has the view of all the Kubernetes clusters involved in setting up the gardener ring will be required to achieve this. However, etcd-druid can be used by such a higher-level component/controller (for example, by supplying the initial cluster configuration) such that individual etcd-druid instances in the individual Kubernetes clusters can manage the corresponding etcd cluster members.</p><h3 id="autonomous-shoot-clusters" tabindex="-1">Autonomous Shoot Clusters <a class="header-anchor" href="#autonomous-shoot-clusters" aria-label="Permalink to &quot;Autonomous Shoot Clusters&quot;">​</a></h3><p><a href="https://github.com/gardener/gardener/issues/2906" target="_blank" rel="noreferrer">Autonomous Shoot Clusters</a> also will require a highly availble etcd cluster to back its control-plane and the multi-node support proposed here can be leveraged in that context. However, the current proposal will not meet all the needs of a autonomous shoot cluster. Some additional components will be required that have the overall view of the autonomous shoot cluster and they can use etcd-druid to manage the multi-node etcd cluster. But this scenario may be different from that of <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring">Gardener Ring</a> in that the individual etcd members of the cluster may not be hosted on different Kubernetes clusters.</p><h3 id="optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data" tabindex="-1">Optimization of recovery from non-quorate cluster with some member containing valid data <a class="header-anchor" href="#optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data" aria-label="Permalink to &quot;Optimization of recovery from non-quorate cluster with some member containing valid data&quot;">​</a></h3><p>It might be possible to optimize the actions during the recovery of a non-quorate cluster where some of the members contain valid data and some other don&#39;t. The optimization involves verifying the data of the valid members to determine the data of which member is the most recent (even considering the latest backup) so that the <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">full snapshot</a> can be taken from it before recovering the etcd cluster. Such an optimization can be attempted in the future.</p><h3 id="optimization-of-rolling-updates-to-unhealthy-etcd-clusters" tabindex="-1">Optimization of rolling updates to unhealthy etcd clusters <a class="header-anchor" href="#optimization-of-rolling-updates-to-unhealthy-etcd-clusters" aria-label="Permalink to &quot;Optimization of rolling updates to unhealthy etcd clusters&quot;">​</a></h3><p>As mentioned <a href="documentation/pr-preview/pr-2/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members">above</a>, optimizations to proceed with rolling updates to unhealthy etcd clusters (without first restoring the cluster to full health) can be pursued in future work.</p>`,342)]))}const g=t(d,[["render",l]]);export{f as __pageData,g as default};
