import{_ as o,c as t,o as a,a2 as r}from"./chunks/framework.B8WFj13S.js";const u=JSON.parse('{"title":"Provider Local","description":"","frontmatter":{"github_repo":"https://github.com/gardener/gardener","github_subdir":"docs/extensions","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/gardener/extensions/provider-local.md","to":"provider-local.md"},"title":"Provider Local","prev":false,"next":false},"headers":[],"relativePath":"docs/gardener/extensions/provider-local/index.md","filePath":"docs/gardener/extensions/provider-local.md","lastUpdated":null}'),n={name:"docs/gardener/extensions/provider-local/index.md"};function s(c,e,i,d,l,h){return a(),t("div",null,e[0]||(e[0]=[r('<h1 id="local-provider-extension" tabindex="-1">Local Provider Extension <a class="header-anchor" href="#local-provider-extension" aria-label="Permalink to &quot;Local Provider Extension&quot;">‚Äã</a></h1><p>The &quot;local provider&quot; extension is used to allow the usage of seed and shoot clusters which run entirely locally without any real infrastructure or cloud provider involved. It implements Gardener&#39;s extension contract (<a href="https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md" target="_blank" rel="noreferrer">GEP-1</a>) and thus comprises several controllers and webhooks acting on resources in seed and shoot clusters.</p><p>The code is maintained in <a href="https://github.com/gardener/gardener/tree/master/pkg/provider-local" target="_blank" rel="noreferrer"><code>pkg/provider-local</code></a>.</p><h2 id="motivation" tabindex="-1">Motivation <a class="header-anchor" href="#motivation" aria-label="Permalink to &quot;Motivation&quot;">‚Äã</a></h2><p>The motivation for maintaining such extension is the following:</p><ul><li>üõ° Output Qualification: Run fast and cost-efficient end-to-end tests, locally and in CI systems (increased confidence ‚õë before merging pull requests)</li><li>‚öôÔ∏è Development Experience: Develop Gardener entirely on a local machine without any external resources involved (improved costs üí∞ and productivity üöÄ)</li><li>ü§ù Open Source: Quick and easy setup for a first evaluation of Gardener and a good basis for first contributions</li></ul><h2 id="current-limitations" tabindex="-1">Current Limitations <a class="header-anchor" href="#current-limitations" aria-label="Permalink to &quot;Current Limitations&quot;">‚Äã</a></h2><p>The following enlists the current limitations of the implementation. Please note that all of them are not technical limitations/blockers, but simply advanced scenarios that we haven&#39;t had invested yet into.</p><ol><li><p>No load balancers for Shoot clusters.</p><p><em>We have not yet developed a <code>cloud-controller-manager</code> which could reconcile load balancer <code>Service</code>s in the shoot cluster.</em></p></li><li><p>In case a seed cluster with multiple availability zones, i.e. multiple entries in <code>.spec.provider.zones</code>, is used in conjunction with a single-zone shoot control plane, i.e. a shoot cluster without <code>.spec.controlPlane.highAvailability</code> or with <code>.spec.controlPlane.highAvailability.failureTolerance.type</code> set to <code>node</code>, the local address of the API server endpoint needs to be determined manually or via the in-cluster <code>coredns</code>.</p><p><em>As the different istio ingress gateway loadbalancers have individual external IP addresses, single-zone shoot control planes can end up in a random availability zone. Having the local host use the <code>coredns</code> in the cluster as name resolver would form a name resolution cycle. The tests mitigate the issue by adapting the DNS configuration inside the affected test.</em></p></li></ol><h2 id="managedseeds" tabindex="-1"><code>ManagedSeed</code>s <a class="header-anchor" href="#managedseeds" aria-label="Permalink to &quot;`ManagedSeed`s&quot;">‚Äã</a></h2><p>It is possible to deploy <a href="documentation/pr-preview/pr-2/docs/gardener/managed_seed/"><code>ManagedSeed</code>s</a> with <code>provider-local</code> by first creating a <a href="https://github.com/gardener/gardener/blob/master/example/provider-local/managedseeds/shoot-managedseed.yaml" target="_blank" rel="noreferrer"><code>Shoot</code> in the <code>garden</code> namespace</a> and then creating a referencing <a href="https://github.com/gardener/gardener/blob/master/example/provider-local/managedseeds/managedseed.yaml" target="_blank" rel="noreferrer"><code>ManagedSeed</code> object</a>.</p><blockquote><p>Please note that this is only supported by the <a href="documentation/pr-preview/pr-2/docs/gardener/deployment/getting_started_locally/"><code>Skaffold</code>-based setup</a>.</p></blockquote><p>The corresponding e2e test can be run via:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">./hack/test-e2e-local.sh</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --label-filter</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;ManagedSeed&quot;</span></span></code></pre></div><h3 id="implementation-details" tabindex="-1">Implementation Details <a class="header-anchor" href="#implementation-details" aria-label="Permalink to &quot;Implementation Details&quot;">‚Äã</a></h3><p>The images locally built by <code>Skaffold</code> for the Gardener components which are deployed to this shoot cluster are managed by a container registry in the <code>registry</code> namespace in the kind cluster. <code>provider-local</code> configures this registry as mirror for the shoot by mutating the <code>OperatingSystemConfig</code> and using the <a href="documentation/pr-preview/pr-2/docs/gardener/advanced/custom-containerd-config/">default contract for extending the <code>containerd</code> configuration</a>.</p><p>In order to bootstrap a seed cluster, the <code>gardenlet</code> deploys <code>PersistentVolumeClaim</code>s and <code>Service</code>s of type <code>LoadBalancer</code>. While storage is supported in shoot clusters by using the <a href="https://github.com/rancher/local-path-provisioner" target="_blank" rel="noreferrer"><code>local-path-provisioner</code></a>, load balancers are not supported yet. However, <code>provider-local</code> runs a <code>Service</code> controller which specifically reconciles the seed-related <code>Service</code>s of type <code>LoadBalancer</code>. This way, they get an IP and <code>gardenlet</code> can finish its bootstrapping process. Note that these IPs are not reachable, however for the sake of developing <code>ManagedSeed</code>s this is sufficient for now.</p><p>Also, please note that the <code>provider-local</code> extension only gets deployed because of the <code>Always</code> deployment policy in its corresponding <code>ControllerRegistration</code> and because the DNS provider type of the seed is set to <code>local</code>.</p><h2 id="implementation-details-1" tabindex="-1">Implementation Details <a class="header-anchor" href="#implementation-details-1" aria-label="Permalink to &quot;Implementation Details&quot;">‚Äã</a></h2><p>This section contains information about how the respective controllers and webhooks in <code>provider-local</code> are implemented and what their purpose is.</p><h3 id="bootstrapping" tabindex="-1">Bootstrapping <a class="header-anchor" href="#bootstrapping" aria-label="Permalink to &quot;Bootstrapping&quot;">‚Äã</a></h3><p>The Helm chart of the <code>provider-local</code> extension defined in its <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/registration/"><code>Extension</code></a> contains a special deployment for a <a href="https://coredns.io/" target="_blank" rel="noreferrer">CoreDNS</a> instance in a <code>gardener-extension-provider-local-coredns</code> namespace in the seed cluster.</p><p>This CoreDNS instance is responsible for enabling the components running in the shoot clusters to be able to resolve the DNS names when they communicate with their <code>kube-apiserver</code>s.</p><p>It contains a static configuration to resolve the DNS names based on <code>local.gardener.cloud</code> to <code>istio-ingressgateway.istio-ingress.svc</code>.</p><h3 id="controllers" tabindex="-1">Controllers <a class="header-anchor" href="#controllers" aria-label="Permalink to &quot;Controllers&quot;">‚Äã</a></h3><p>There are controllers for all resources in the <code>extensions.gardener.cloud/v1alpha1</code> API group except for <code>BackupBucket</code> and <code>BackupEntry</code>s.</p><h4 id="controlplane" tabindex="-1"><code>ControlPlane</code> <a class="header-anchor" href="#controlplane" aria-label="Permalink to &quot;`ControlPlane`&quot;">‚Äã</a></h4><p>This controller is deploying the <a href="https://github.com/rancher/local-path-provisioner" target="_blank" rel="noreferrer">local-path-provisioner</a> as well as a related <code>StorageClass</code> in order to support <code>PersistentVolumeClaim</code>s in the local shoot cluster. Additionally, it creates a few (currently unused) dummy secrets (CA, server and client certificate, basic auth credentials) for the sake of testing the secrets manager integration in the extensions library.</p><h4 id="dnsrecord" tabindex="-1"><code>DNSRecord</code> <a class="header-anchor" href="#dnsrecord" aria-label="Permalink to &quot;`DNSRecord`&quot;">‚Äã</a></h4><p>The controller adapts the cluster internal DNS configuration by extending the <code>coredns</code> configuration for every observed <code>DNSRecord</code>. It will add two corresponding entries in the custom DNS configuration per shoot cluster:</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>data:</span></span>\n<span class="line"><span>  api.local.local.external.local.gardener.cloud.override: |</span></span>\n<span class="line"><span>    rewrite stop name regex api.local.local.external.local.gardener.cloud istio-ingressgateway.istio-ingress.svc.cluster.local answer auto</span></span>\n<span class="line"><span>  api.local.local.internal.local.gardener.cloud.override: |</span></span>\n<span class="line"><span>    rewrite stop name regex api.local.local.internal.local.gardener.cloud istio-ingressgateway.istio-ingress.svc.cluster.local answer auto</span></span></code></pre></div><h4 id="infrastructure" tabindex="-1"><code>Infrastructure</code> <a class="header-anchor" href="#infrastructure" aria-label="Permalink to &quot;`Infrastructure`&quot;">‚Äã</a></h4><p>This controller generates a <code>NetworkPolicy</code> which allows the control plane pods (like <code>kube-apiserver</code>) to communicate with the worker machine pods (see <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/provider-local/#worker"><code>Worker</code> section</a>). It also deploys a <code>NetworkPolicy</code> which allows the bastion pods to communicate with the worker machine pods (see <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/provider-local/#bastion"><code>Bastion</code> section</a>).</p><h4 id="network" tabindex="-1"><code>Network</code> <a class="header-anchor" href="#network" aria-label="Permalink to &quot;`Network`&quot;">‚Äã</a></h4><p>This controller is not implemented anymore. In the initial version of <code>provider-local</code>, there was a <code>Network</code> controller deploying <a href="https://github.com/kubernetes-sigs/kind/blob/main/images/kindnetd/README.md" target="_blank" rel="noreferrer">kindnetd</a> (see <a href="https://github.com/gardener/gardener/tree/v1.44.1/pkg/provider-local/controller/network" target="_blank" rel="noreferrer">release v1.44.1</a>). However, we decided to drop it because this setup prevented us from using <code>NetworkPolicy</code>s (kindnetd does not ship a <code>NetworkPolicy</code> controller). In addition, we had issues with shoot clusters having more than one node (hence, we couldn&#39;t support rolling updates, see <a href="https://github.com/gardener/gardener/pull/5666/commits/491b3cd16e40e5c20ef02367fda93a34ff9465eb" target="_blank" rel="noreferrer">PR #5666</a>).</p><h4 id="operatingsystemconfig" tabindex="-1"><code>OperatingSystemConfig</code> <a class="header-anchor" href="#operatingsystemconfig" aria-label="Permalink to &quot;`OperatingSystemConfig`&quot;">‚Äã</a></h4><p>This controller renders a simple cloud-init template which can later be executed by the shoot worker nodes.</p><p>The shoot worker nodes are <code>Pod</code>s with a container based on the <code>kindest/node</code> image. This is maintained in the <a href="https://github.com/gardener/machine-controller-manager-provider-local/tree/master/node" target="_blank" rel="noreferrer">gardener/machine-controller-manager-provider-local repository</a> and has a special <code>run-userdata</code> systemd service which executes the cloud-init generated earlier by the <code>OperatingSystemConfig</code> controller.</p><h4 id="worker" tabindex="-1"><code>Worker</code> <a class="header-anchor" href="#worker" aria-label="Permalink to &quot;`Worker`&quot;">‚Äã</a></h4><p>This controller leverages the standard <a href="https://github.com/gardener/gardener/tree/master/extensions/pkg/controller/worker/genericactuator" target="_blank" rel="noreferrer">generic <code>Worker</code> actuator</a> in order to generate the <a href="https://github.com/gardener/machine-controller-manager-provider-local/blob/master/kubernetes/machine-class.yaml" target="_blank" rel="noreferrer"><code>MachineClass</code>es</a> and the <code>MachineDeployment</code>s based on the specification of the <code>Worker</code> resource.</p><p>Additionally, the controller deploys RBAC objects for granting machine-controller-manager additional permissions in the control plane namespace. This is needed because the <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/provider-local/#machine-controller-manager-provider-local">local machine provider</a> creates Kubernetes objects in the control plane namespace for starting <code>Machines</code>, which is different to all other machine provider implementations.</p><h4 id="bastion" tabindex="-1"><code>Bastion</code> <a class="header-anchor" href="#bastion" aria-label="Permalink to &quot;`Bastion`&quot;">‚Äã</a></h4><p>This controller implements the <code>Bastion.extensions.gardener.cloud</code> resource by deploying a pod with the local machine image along with a <code>LoadBalancer</code> service.</p><p>Note that this controller does not respect the <code>Bastion.spec.ingress</code> configuration as there is no way to perform client IP restrictions in the local setup.</p><h4 id="ingress" tabindex="-1"><code>Ingress</code> <a class="header-anchor" href="#ingress" aria-label="Permalink to &quot;`Ingress`&quot;">‚Äã</a></h4><p>The gardenlet creates a wildcard DNS record for the Seed&#39;s ingress domain pointing to the <code>nginx-ingress-controller</code>&#39;s LoadBalancer. This domain is commonly used by all <code>Ingress</code> objects created in the Seed for Seed and Shoot components. As provider-local implements the <code>DNSRecord</code> extension API (see the <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/provider-local/#dnsrecord"><code>DNSRecord</code>section</a>), this controller reconciles all <code>Ingress</code>s and creates <code>DNSRecord</code>s of type <code>local</code> for each host included in <code>spec.rules</code>. This only happens for shoot namespaces (<code>gardener.cloud/role=shoot</code> label) to make <code>Ingress</code> domains resolvable on the machine pods.</p><h4 id="service" tabindex="-1"><code>Service</code> <a class="header-anchor" href="#service" aria-label="Permalink to &quot;`Service`&quot;">‚Äã</a></h4><p>This controller reconciles <code>Services</code> of type <code>LoadBalancer</code> in the local <code>Seed</code> cluster. Since the local Kubernetes clusters used as Seed clusters typically don&#39;t support such services, this controller sets the <code>.status.ingress.loadBalancer.ip[0]</code> to the IP of the host. It makes important LoadBalancer Services (e.g. <code>istio-ingress/istio-ingressgateway</code> and <code>shoot--*--*/bastion-*</code>) available to the host by setting <code>spec.ports[].nodePort</code> to well-known ports that are mapped to <code>hostPorts</code> in the kind cluster configuration.</p><p><code>istio-ingress/istio-ingressgateway</code> is set to be exposed on <code>nodePort</code> <code>30433</code> by this controller. The bastion services are exposed on <code>nodePort</code> <code>30022</code>.</p><p>In case the seed has multiple availability zones (<code>.spec.provider.zones</code>) and it uses SNI, the different zone-specific <code>istio-ingressgateway</code> loadbalancers are exposed via different IP addresses. Per default, IP addresses <code>172.18.255.10</code>, <code>172.18.255.11</code>, and <code>172.18.255.12</code> are used for the zones <code>0</code>, <code>1</code>, and <code>2</code> respectively.</p><h4 id="etcd-backups" tabindex="-1">ETCD Backups <a class="header-anchor" href="#etcd-backups" aria-label="Permalink to &quot;ETCD Backups&quot;">‚Äã</a></h4><p>This controller reconciles the <code>BackupBucket</code> and <code>BackupEntry</code> of the shoot allowing the <code>etcd-backup-restore</code> to create and copy backups using the <code>local</code> provider functionality. The backups are stored on the host file system. This is achieved by mounting that directory to the <code>etcd-backup-restore</code> container.</p><h4 id="extension-seed" tabindex="-1">Extension Seed <a class="header-anchor" href="#extension-seed" aria-label="Permalink to &quot;Extension Seed&quot;">‚Äã</a></h4><p>This controller reconciles <code>Extensions</code> of type <code>local-ext-seed</code>. It creates a single <code>serviceaccount</code> named <code>local-ext-seed</code> in the shoot&#39;s namespace in the seed. The extension is reconciled before the <code>kube-apiserver</code>. More on extension lifecycle strategies can be read in <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/registration/#extension-lifecycle">Registering Extension Controllers</a>.</p><h4 id="extension-shoot" tabindex="-1">Extension Shoot <a class="header-anchor" href="#extension-shoot" aria-label="Permalink to &quot;Extension Shoot&quot;">‚Äã</a></h4><p>This controller reconciles <code>Extensions</code> of type <code>local-ext-shoot</code>. It creates a single <code>serviceaccount</code> named <code>local-ext-shoot</code> in the <code>kube-system</code> namespace of the shoot. The extension is reconciled after the <code>kube-apiserver</code>. More on extension lifecycle strategies can be read <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/registration/#extension-lifecycle">Registering Extension Controllers</a>.</p><h4 id="extension-shoot-after-worker" tabindex="-1">Extension Shoot After Worker <a class="header-anchor" href="#extension-shoot-after-worker" aria-label="Permalink to &quot;Extension Shoot After Worker&quot;">‚Äã</a></h4><p>This controller reconciles <code>Extensions</code> of type <code>local-ext-shoot-after-worker</code>. It creates a <code>deployment</code> named <code>local-ext-shoot-after-worker</code> in the <code>kube-system</code> namespace of the shoot. The extension is reconciled after the workers and waits until the deployment is ready. More on extension lifecycle strategies can be read <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/registration/#extension-lifecycle">Registering Extension Controllers</a>.</p><h4 id="health-checks" tabindex="-1">Health Checks <a class="header-anchor" href="#health-checks" aria-label="Permalink to &quot;Health Checks&quot;">‚Äã</a></h4><p>The health check controller leverages the <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/healthcheck-library/">health check library</a> in order to:</p><ul><li>check the health of the <code>ManagedResource/extension-controlplane-shoot-webhooks</code> and populate the <code>SystemComponentsHealthy</code> condition in the <code>ControlPlane</code> resource.</li><li>check the health of the <code>ManagedResource/extension-networking-local</code> and populate the <code>SystemComponentsHealthy</code> condition in the <code>Network</code> resource.</li><li>check the health of the <code>ManagedResource/extension-worker-mcm-shoot</code> and populate the <code>SystemComponentsHealthy</code> condition in the <code>Worker</code> resource.</li><li>check the health of the <code>Deployment/machine-controller-manager</code> and populate the <code>ControlPlaneHealthy</code> condition in the <code>Worker</code> resource.</li><li>check the health of the <code>Node</code>s and populate the <code>EveryNodeReady</code> condition in the <code>Worker</code> resource.</li></ul><h3 id="webhooks" tabindex="-1">Webhooks <a class="header-anchor" href="#webhooks" aria-label="Permalink to &quot;Webhooks&quot;">‚Äã</a></h3><h4 id="control-plane" tabindex="-1">Control Plane <a class="header-anchor" href="#control-plane" aria-label="Permalink to &quot;Control Plane&quot;">‚Äã</a></h4><p>This webhook reacts on the <code>OperatingSystemConfig</code> containing the configuration of the kubelet and sets the <code>failSwapOn</code> to <code>false</code> (independent of what is configured in the <code>Shoot</code> spec) (<a href="https://github.com/kubernetes-sigs/kind/blob/b6bc112522651d98c81823df56b7afa511459a3b/site/content/docs/design/node-image.md#design" target="_blank" rel="noreferrer">ref</a>).</p><h4 id="dns-config" tabindex="-1">DNS Config <a class="header-anchor" href="#dns-config" aria-label="Permalink to &quot;DNS Config&quot;">‚Äã</a></h4><p>This webhook reacts on events for the <code>dependency-watchdog-probe</code> <code>Deployment</code>, the <code>blackbox-exporter</code> <code>Deployment</code>, as well as on events for <code>Pod</code>s created when the <code>machine-controller-manager</code> reconciles <code>Machine</code>s. All these pods need to be able to resolve the DNS names for shoot clusters. It sets the <code>.spec.dnsPolicy=None</code> and <code>.spec.dnsConfig.nameServers</code> to the cluster IP of the <code>coredns</code> <code>Service</code> created in the <code>gardener-extension-provider-local-coredns</code> namespaces so that these pods can resolve the DNS records for shoot clusters (see the <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/provider-local/#bootstrapping">Bootstrapping section</a> for more details).</p><h4 id="node" tabindex="-1">Node <a class="header-anchor" href="#node" aria-label="Permalink to &quot;Node&quot;">‚Äã</a></h4><p>This webhook reacts on updates to <code>nodes/status</code> in both seed and shoot clusters and sets the <code>.status.{allocatable,capacity}.cpu=&quot;100&quot;</code> and <code>.status.{allocatable,capacity}.memory=&quot;100Gi&quot;</code> fields.</p><p>Background: Typically, the <code>.status.{capacity,allocatable}</code> values are determined by the resources configured for the Docker daemon (see for example the <a href="https://docs.docker.com/desktop/mac/#resources" target="_blank" rel="noreferrer">docker Quick Start Guide</a> for Mac). Since many of the <code>Pod</code>s deployed by Gardener have quite high <code>.spec.resources.requests</code>, the <code>Node</code>s easily get filled up and only a few <code>Pod</code>s can be scheduled (even if they barely consume any of their reserved resources). In order to improve the user experience, on startup/leader election the provider-local extension submits an empty patch which triggers the &quot;node webhook&quot; (see the below section) for the seed cluster. The webhook will increase the capacity of the <code>Node</code>s to allow all <code>Pod</code>s to be scheduled. For the shoot clusters, this empty patch trigger is not needed since the <code>MutatingWebhookConfiguration</code> is reconciled by the <code>ControlPlane</code> controller and exists before the <code>Node</code> object gets registered.</p><h4 id="shoot" tabindex="-1">Shoot <a class="header-anchor" href="#shoot" aria-label="Permalink to &quot;Shoot&quot;">‚Äã</a></h4><p>This webhook reacts on the <code>ConfigMap</code> used by the <code>kube-proxy</code> and sets the <code>maxPerCore</code> field to <code>0</code> since other values don&#39;t work well in conjunction with the <code>kindest/node</code> image which is used as base for the shoot worker machine pods (<a href="https://github.com/kubernetes-sigs/kind/blob/fa7d86470f4c0e924fc4c2e767ec8491c45f4304/pkg/cluster/internal/kubeadm/config.go#L283-L285" target="_blank" rel="noreferrer">ref</a>).</p><h3 id="dns-configuration-for-multi-zonal-seeds" tabindex="-1">DNS Configuration for Multi-Zonal Seeds <a class="header-anchor" href="#dns-configuration-for-multi-zonal-seeds" aria-label="Permalink to &quot;DNS Configuration for Multi-Zonal Seeds&quot;">‚Äã</a></h3><p>In case a seed cluster has multiple availability zones as specified in <code>.spec.provider.zones</code>, multiple istio ingress gateways are deployed, one per availability zone in addition to the default deployment. The result is that single-zone shoot control planes, i.e. shoot clusters with <code>.spec.controlPlane.highAvailability</code> set or with <code>.spec.controlPlane.highAvailability.failureTolerance.type</code> set to <code>node</code>, may be exposed via any of the zone-specific istio ingress gateways. Previously, the endpoints were statically mapped via <code>/etc/hosts</code>. Unfortunately, this is no longer possible due to the aforementioned dynamic in the endpoint selection.</p><p>For multi-zonal seed clusters, there is an additional configuration following <code>coredns</code>&#39;s <a href="https://github.com/coredns/coredns/tree/master/plugin/view" target="_blank" rel="noreferrer">view plugin</a> mapping the external IP addresses of the zone-specific loadbalancers to the corresponding internal istio ingress gateway domain names. This configuration is only in place for requests from outside of the seed cluster. Those requests are currently being identified by the protocol. UDP requests are interpreted as originating from within the seed cluster while TCP requests are assumed to come from outside the cluster via the docker hostport mapping.</p><p>The corresponding test sets the DNS configuration accordingly so that the name resolution during the test use <code>coredns</code> in the cluster.</p><h3 id="machine-controller-manager-provider-local" tabindex="-1">machine-controller-manager-provider-local <a class="header-anchor" href="#machine-controller-manager-provider-local" aria-label="Permalink to &quot;machine-controller-manager-provider-local&quot;">‚Äã</a></h3><p>Out of tree (controller-based) implementation for <code>local</code> as a new provider. The local out-of-tree provider implements the interface defined at <a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/driver/driver.go" target="_blank" rel="noreferrer">MCM OOT driver</a>.</p><p>For every <code>Machine</code> object, the <a href="https://github.com/gardener/gardener/tree/master/pkg/provider-local/machine-provider" target="_blank" rel="noreferrer">local machine provider</a> creates a <code>Pod</code> in the shoot control plane namespace. A machine pod uses an <a href="https://github.com/gardener/gardener/tree/master/pkg/provider-local/node" target="_blank" rel="noreferrer">image</a> based on <a href="https://github.com/kubernetes-sigs/kind" target="_blank" rel="noreferrer">kind&#39;s</a> node image (<code>kindest/node</code>). The machine&#39;s user data is deployed as a <code>Secret</code> in the shoot control plane namespace and mounted into the machine pod. In contrast to kind, the local machine image doesn&#39;t directly run kubelet as a systemd unit. Instead, it has a unit for running the user data script at <code>/etc/machine/userdata</code>.</p><p>Typically, machines running in a cloud infrastructure environment can resolve the hostnames of other machines in the same cluster/network. To mimic this behavior in the local setup, the machine provider creates a <code>Service</code> for every <code>Machine</code> with the same name as the <code>Pod</code>. With this, local <code>Nodes</code> and <code>Bastions</code> can connect to other <code>Nodes</code> via their hostname.</p><h2 id="future-work" tabindex="-1">Future Work <a class="header-anchor" href="#future-work" aria-label="Permalink to &quot;Future Work&quot;">‚Äã</a></h2><p>Future work could mostly focus on resolving the above listed <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/provider-local/#limitations">limitations</a>, i.e.:</p><ul><li>Implement a <code>cloud-controller-manager</code> and deploy it via the <a href="documentation/pr-preview/pr-2/docs/gardener/extensions/provider-local/#controlplane"><code>ControlPlane</code> controller</a>.</li><li>Properly implement <code>.spec.machineTypes</code> in the <code>CloudProfile</code>s (i.e., configure <code>.spec.resources</code> properly for the created shoot worker machine pods).</li></ul>',82)]))}const m=o(n,[["render",s]]);export{u as __pageData,m as default};
