import{_ as o}from"./chunks/logging-architecture.DUXxo7HC.js";import{_ as t,c as s,o as n,a2 as a}from"./chunks/framework.Bfq10Vlj.js";const i="/assets/shoot-node-logging-architecture.CqDK0Gei.png",r="/assets/explore-button-usage.Ce0fTcLC.png",b=JSON.parse('{"title":"Logging","description":"","frontmatter":{"aliases":["/docs/gardener/logging/"],"github_repo":"https://github.com/gardener/gardener","github_subdir":"docs/usage/observability","params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/docs/gardener/observability/logging.md","to":"logging.md"},"persona":"Users","title":"Logging","prev":false,"next":false},"headers":[],"relativePath":"docs/gardener/observability/logging/index.md","filePath":"docs/gardener/observability/logging.md","lastUpdated":null}'),l={name:"docs/gardener/observability/logging/index.md"};function c(d,e,h,p,g,u){return n(),s("div",null,e[0]||(e[0]=[a('<h1 id="logging-stack" tabindex="-1">Logging Stack <a class="header-anchor" href="#logging-stack" aria-label="Permalink to &quot;Logging Stack&quot;">​</a></h1><h2 id="motivation" tabindex="-1">Motivation <a class="header-anchor" href="#motivation" aria-label="Permalink to &quot;Motivation&quot;">​</a></h2><p>Kubernetes uses the underlying container runtime logging, which does not persist logs for stopped and destroyed containers. This makes it difficult to investigate issues in the very common case of not running containers. Gardener provides a solution to this problem for the managed cluster components by introducing its own logging stack.</p><h2 id="components" tabindex="-1">Components <a class="header-anchor" href="#components" aria-label="Permalink to &quot;Components&quot;">​</a></h2><ul><li>A Fluent-bit daemonset which works like a log collector and custom Golang plugin which spreads log messages to their Vali instances.</li><li>One Vali Statefulset in the <code>garden</code> namespace which contains logs for the seed cluster and one per shoot namespace which contains logs for shoot&#39;s controlplane.</li><li>One Plutono Deployment in <code>garden</code> namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators). Plutono is the UI component used in the logging stack.</li></ul><p><img src="'+o+`" alt=""></p><h2 id="container-logs-rotation-and-retention-in-kubelet" tabindex="-1">Container Logs Rotation and Retention in kubelet <a class="header-anchor" href="#container-logs-rotation-and-retention-in-kubelet" aria-label="Permalink to &quot;Container Logs Rotation and Retention in kubelet&quot;">​</a></h2><p>It is possible to configure the <code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code> fields in the Shoot specification. Both fields are optional and if nothing is specified, then the <code>kubelet</code> rotates on the size <code>100M</code>. Those fields are part of provider&#39;s workers definition. Here is an example:</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  provider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    workers</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      - </span><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">cri</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          name</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">containerd</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">        kubernetes</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">          kubelet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # accepted values are of resource.Quantity</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">            containerLogMaxSize</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">150Mi</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">            containerLogMaxFiles</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span></span></code></pre></div><p>The values of the <code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code> fields need to be considered with care since container log files claim disk space from the host. On the opposite side, log rotations on too small sizes may result in frequent rotations which can be missed by other components (log shippers) observing these rotations.</p><p>In the majority of the cases, the defaults should do just fine. Custom configuration might be of use under rare conditions.</p><h2 id="logs-retention-in-vali" tabindex="-1">Logs Retention in Vali <a class="header-anchor" href="#logs-retention-in-vali" aria-label="Permalink to &quot;Logs Retention in Vali&quot;">​</a></h2><p>Logs in Vali are preserved for a maximum of 14 days. Note that the retention period is also restricted to the Vali&#39;s persistent volume size and in some cases, it can be less than 14 days. The oldest logs are deleted when a configured threshold of free disk space is crossed.</p><h2 id="extension-of-the-logging-stack" tabindex="-1">Extension of the Logging Stack <a class="header-anchor" href="#extension-of-the-logging-stack" aria-label="Permalink to &quot;Extension of the Logging Stack&quot;">​</a></h2><p>The logging stack is extended to scrape logs from the systemd services of each shoots&#39; nodes and from all Gardener components in the shoot <code>kube-system</code> namespace. These logs are exposed only to the Gardener operators.</p><p>Also, in the shoot control plane an <code>event-logger</code> pod is deployed, which scrapes events from the shoot <code>kube-system</code> namespace and shoot <code>control-plane</code> namespace in the seed. The <code>event-logger</code> logs the events to the standard output. Then the <code>fluent-bit</code> gets these events as container logs and sends them to the Vali in the shoot control plane (similar to how it works for any other control plane component). <img src="`+i+'" alt=""></p><h2 id="how-to-access-the-logs" tabindex="-1">How to Access the Logs <a class="header-anchor" href="#how-to-access-the-logs" aria-label="Permalink to &quot;How to Access the Logs&quot;">​</a></h2><p>The logs are accessible via Plutono. To access them:</p><ol><li><p>Authenticate via basic auth to gain access to Plutono.<br> The secret containing the credentials is stored in the project namespace following the naming pattern <code>&lt;shoot-name&gt;.monitoring</code>. In this secret you can also find the Plutono URL in the <code>plutono-url</code> annotation. For Gardener operators, the credentials are also stored in the control-plane (<code>shoot--&lt;project-name&gt;--&lt;shoot-name&gt;</code>) namespace in the <code>observability-ingress-users-&lt;hash&gt;</code> secret in the seed.</p></li><li><p>Plutono contains several dashboards that aim to facilitate the work of operators and users. From the <code>Explore</code> tab, users and operators have unlimited abilities to extract and manipulate logs.</p></li></ol><blockquote><p><strong>Note:</strong> Gardener Operators are people part of the Gardener team with operator permissions, not operators of the end-user cluster!</p></blockquote><h3 id="how-to-use-the-explore-tab" tabindex="-1">How to Use the <code>Explore</code> Tab <a class="header-anchor" href="#how-to-use-the-explore-tab" aria-label="Permalink to &quot;How to Use the `Explore` Tab&quot;">​</a></h3><p>If you click on the <code>Log browser &gt;</code> button, you will see all of the available labels. Clicking on the label, you can see all of its available values for the given period of time you have specified. If you are searching for logs for the past one hour, do not expect to see labels or values for which there were no logs for that period of time. By clicking on a value, Plutono automatically eliminates all other labels and/or values with which no valid log stream can be made. After choosing the right labels and their values, click on the <code>Show logs</code> button. This will build <code>Log query</code> and execute it. This approach is convenient when you don&#39;t know the labels names or they values. <img src="'+r+'" alt=""></p><p>Once you feel comfortable, you can start to use the <a href="https://github.com/credativ/plutono" target="_blank" rel="noreferrer">LogQL</a> language to search for logs. Next to the <code>Log browser &gt;</code> button is the place where you can type log queries.</p><p>Examples:</p><ol><li><p>If you want to get logs for <code>calico-node-&lt;hash&gt;</code> pod in the cluster <code>kube-system</code>: The name of the node on which <code>calico-node</code> was running is known, but not the hash suffix of the <code>calico-node</code> pod. Also we want to search for errors in the logs.</p><p><code>{pod_name=~&quot;calico-node-.+&quot;, nodename=&quot;ip-10-222-31-182.eu-central-1.compute.internal&quot;} |~ &quot;error&quot;</code></p><p>Here, you will get as much help as possible from the Plutono by giving you suggestions and auto-completion.</p></li><li><p>If you want to get the logs from <code>kubelet</code> systemd service of a given node and search for a pod name in the logs:</p><p><code>{unit=&quot;kubelet.service&quot;, nodename=&quot;ip-10-222-31-182.eu-central-1.compute.internal&quot;} |~ &quot;pod name&quot;</code></p></li></ol><blockquote><p><strong>Note:</strong> Under <code>unit</code> label there is only the <code>docker</code>, <code>containerd</code>, <code>kubelet</code> and <code>kernel</code> logs.</p></blockquote><ol start="3"><li><p>If you want to get the logs from <code>gardener-node-agent</code> systemd service of a given node and search for a string in the logs:</p><p><code>{job=&quot;systemd-combine-journal&quot;,nodename=&quot;ip-10-222-31-182.eu-central-1.compute.internal&quot;} | unpack | unit=&quot;gardener-node-agent.service&quot;</code></p></li></ol><blockquote><p><strong>Note:</strong> <code>{job=&quot;systemd-combine-journal&quot;,nodename=&quot;&lt;node name&gt;&quot;}</code> stream <a href="https://github.com/credativ/plutono" target="_blank" rel="noreferrer">pack</a> all logs from systemd services except <code>docker</code>, <code>containerd</code>, <code>kubelet</code>, and <code>kernel</code>. To filter those log by unit, you have to <a href="https://github.com/credativ/plutono" target="_blank" rel="noreferrer">unpack</a> them first.</p></blockquote><ol start="4"><li>Retrieving events:</li></ol><ul><li><p>If you want to get the events from the shoot <code>kube-system</code> namespace generated by <code>kubelet</code> and related to the <code>node-problem-detector</code>:</p><p><code>{job=&quot;event-logging&quot;} | unpack | origin_extracted=&quot;shoot&quot;,source=&quot;kubelet&quot;,object=~&quot;.*node-problem-detector.*&quot;</code></p></li><li><p>If you want to get the events generated by MCM in the shoot control plane in the seed:</p><p><code>{job=&quot;event-logging&quot;} | unpack | origin_extracted=&quot;seed&quot;,source=~&quot;.*machine-controller-manager.*&quot;</code></p><blockquote><p><strong>Note:</strong> In order to group events by origin, one has to specify <code>origin_extracted</code> because the <code>origin</code> label is reserved for all of the logs from the seed and the <code>event-logger</code> resides in the seed, so all of its logs are coming as they are only from the seed. The actual origin is embedded in the unpacked event. When unpacked, the embedded <code>origin</code> becomes <code>origin_extracted</code>.</p></blockquote></li></ul>',30)]))}const f=t(l,[["render",c]]);export{b as __pageData,f as default};
